<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ye Yang</title>
    <description>My Personal Blog</description>
    <link>http://localhost:4000https://ictyangye.github.io/</link>
    <atom:link href="http://localhost:4000https://ictyangye.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 08 Apr 2019 09:46:32 +0800</pubDate>
    <lastBuildDate>Mon, 08 Apr 2019 09:46:32 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>同步互斥机制（锁、原子操作、内存屏障和无锁实现）</title>
        <description>&lt;p&gt;在计算机程序中的同步互斥就像生活中随处可见的排队等服务一样，像在医院、餐厅、机场等，都会有资源不足的情况，&lt;strong&gt;在同一时间能够提供的服务远低于需求量，而且同一时间只能为有限个客户提供服务&lt;/strong&gt;，这就需要人们遵守规则等待，不能打起来。这就是最为人们熟知的就是生产者-消费者问题，也有“读者-写者”等若干个版本，但它们本质都是一样为了解决共享资源里的同步和互斥问题。&lt;/p&gt;

&lt;p&gt;这里不会像大多数操作系统教程里面那样从锁、信号量、互斥量开始讲解解决同步互斥问题的基本原则和方法，我想从一般linux编程的角度，谈谈在项目中大多如何解决这类问题，以及影响效率、资源使用的因素是什么。&lt;/p&gt;

&lt;h1 id=&quot;1锁&quot;&gt;1.锁&lt;/h1&gt;

&lt;p&gt;解决对共享资源使用时发生冲突的方式，最直接但有效的方式就是加锁，这就像我们的一医一患的诊室一样，为用户提供服务时，锁上门，将其他等待的用户阻止在门外。在大型linux项目中使用的比较常见的锁有，&lt;strong&gt;自旋锁(spinlock)、读写锁(rwlock)和RCU(Read-Copy Update)&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;spinlock&quot;&gt;spinlock&lt;/h3&gt;

&lt;p&gt;对于自旋锁的概念有一段来自百度百科的解释：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;何谓自旋锁？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过这段介绍我们也可以很清楚地看到&lt;strong&gt;自旋锁的缺点：CPU资源利用率低&lt;/strong&gt;，因为一直检查锁的状态，所以只适用于短时间等待的场景，否则造成CPU资源利用率降低。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;它的另一个缺点是不正确的使用会引发死锁&lt;/strong&gt;，事实上这不仅仅是自旋锁的问题，所有的同步互斥解决方法不正确使用都会引发这个问题。但在自旋锁中最常见的情况是递归使用一个自旋锁，即如果一个已经拥有某个自旋锁的CPU 想第二次获得这个自旋锁，则该CPU 将死锁。此外，如果进程获得自旋锁之后再阻塞，也有可能导致死锁的发生。&lt;code class=&quot;highlighter-rouge&quot;&gt;copy_from_user()&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;copy_to_user()&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;kmalloc()&lt;/code&gt;等函数都有可能引起阻塞，因此在自旋锁的占用期间不能调用这些函数[1]。&lt;/p&gt;

&lt;p&gt;在linux内核函数中，自旋锁的实现与体系结构硬相关，在对于体系结构的&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;asm/spinlock.h&amp;gt;&lt;/code&gt;中可以找到相关代码。其基本使用方式非常简单：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spinlock_t lock = SPIN_LOCK_UNLOCKED;
spin_lock(&amp;amp;lock);
/*临界区业务逻辑代码*/
spin_unlock(&amp;amp;lock);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;自旋锁的加解锁API远不止这两个，其解决的是多核情况下多个CPU之间的竞争问题，而单CPU模式下不需要自旋锁，这也意味着它的实现有很多涉及硬件指令级的操作，像中断还有&lt;strong&gt;CAS(compare and swap)&lt;/strong&gt;，CAS是有名的无锁实现，这会在最后章节里面讲。&lt;/p&gt;

&lt;p&gt;其他的API可以查看对应的文档，&lt;strong&gt;直接使用linux自旋锁有两点要注意：
1）自旋锁是不可递归的（原因上面讲过）
2）线程获取自旋锁之前，要禁止当前处理器上的中断。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;上述第二点有这样一种情况，当一个线程获取了自旋锁以后，在临界区中被中断处理程序打断，中断处理程序正好也要获取这个锁，而造成中断处理程序和当前线程互相等待的死锁。&lt;/p&gt;

&lt;h3 id=&quot;rwlock&quot;&gt;rwlock&lt;/h3&gt;

&lt;p&gt;在一些很常见的操作中，会出现&lt;strong&gt;频繁地读取一个变量，但是写操作很少&lt;/strong&gt;的情况。例如，OVS中对流表的读取，每个数据包都会读取流表进行五元组匹配查找，但是只有有限情况下，像手动增删流表项，才会触发对流表结构的写操作。这种情况下，对于多读操作是可以并行的，互不干扰，但是写与读互斥。这种场景下一般使用一种特殊的自旋锁，读写锁来实现。&lt;/p&gt;

&lt;p&gt;上面的描述基本概况了&lt;strong&gt;读写锁的几个特性：
1）读操作资源共享
2）写操作之间互斥
3）写操作与读操作互斥&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这在大多操作系统教程中用的是读者-写者问题来描述这一事实，并且针对读者优先级更高还是写者优先级更高指定不同的策略。&lt;/p&gt;

&lt;p&gt;在Linux实现中，与自旋锁类似，读写锁rwlock的相关API在&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;asm/rwlock.h&amp;gt;&lt;/code&gt;中，使用方法上也与普通自旋锁类似。&lt;/p&gt;

&lt;h3 id=&quot;rcu&quot;&gt;RCU&lt;/h3&gt;

&lt;p&gt;RCU(Read-Copy Update)也是在大型项目中常用的一种读写锁，它的出现主要是为了解决上述普通读写锁中，&lt;strong&gt;写进程对临界区写操作时，阻塞了所有的读操作&lt;/strong&gt;这一问题。试想一下，在OVS流表匹配中，如果一个写操作引发了所有的读阻塞，那是不是那一时刻流量中断了？这将引发严重的问题。所以&lt;strong&gt;RCU的设计就是为了让写操作不要阻塞读操作&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;有篇博客对RCU的分析总结非常透彻[2]，其中总结了&lt;strong&gt;RCU有三个要素：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1）读标志。如果一个Reader企图占据一把RCU锁，它是不需要付出任何代价的，只需要设置一个标志，让外界知道有Reader在占据这把RCU锁，多个Reader可以共同持有一把RCU锁。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2）写时拷贝。如果有一个Write企图更新RCU锁所保护的数据，那么它会首先查看该RCU锁的读标志，如果有该标志，说明有最少一个Reader持有了该RCU锁，它需要对原始数据make a copy，写这个副本并将更新过的副本保存在某处，等待时机用该副本更新原始数据。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3）更新时机。这个时机就是用副本更新原始数据的时间点，这个时间点如何确定是RCU锁实现的算法核心，它直接可以确定所有的数据结构。确切来讲，Writer必须 waitting for all readers leaving，方可Update原始数据。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这在linux具体实现中也有很多版本迭代，有利于抢占禁止的、阶段计数器的。但是最基本的机理就如同上面三要素所说。可以说所有关于多读少写的场景都可以利用RCU获取连续不中断地运行。&lt;/p&gt;

&lt;h1 id=&quot;2原子操作&quot;&gt;2.原子操作&lt;/h1&gt;

&lt;p&gt;原子本意是不可再分的粒子，因此在操作系统中用此名字来命名最基本的操作单一，即&lt;strong&gt;如果一个线程执行原子操作，要么完全执行完，要么完全没有开始执行，期间不会被任何别的线程打断&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;原子操作&quot;&gt;原子操作&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;原子操作依赖于硬件处理器实现，早在单处理器时代，原子操作被认为是单条指令；而在我们今天越来越复杂的多核CPU时代，即使是运行单条指令也不能保证它不会被干扰，因为多核CPU不可避免的共享总线，而一条指令或许就伴随着访存操作等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;在x86平台上，CPU提供三种独立的原子锁机制：原子保证操作、加LOCK指令前缀和缓存一致性协议。&lt;/strong&gt;其中原子保证操作一般用于基础内存事务，比方说一个字节的读写或者对于边界对齐是字节、字、双字、四字等读写都可以保证是原子操作。加LOCK指令前缀主要是一种总线锁，其原始实现是当前CPU拉低总线电平锁住总线，而后不断完善性能已有了新的方式代替，但实现的功能还是一样。而缓存一致性协议，又被称为是MESI协议，由于我们CPU有自己的cache缓存，而存在内存中的数据可能会被多个CPU利用，所以需要一种防止多个处理器同时修改相同内存地址的方式。&lt;/p&gt;

&lt;p&gt;关于cache的MESI协议可以参考任何计算机体系结构教材。这里需要特别提到一个指令&lt;strong&gt;CMPXCHG，它的语义是实现比较并交换操作数（CAS，Compare And Set）&lt;/strong&gt;。CAS操作需要输入两个数值，一个旧值（期望操作前的值）和一个新值，在操作期间先比较下旧值有没有发生变化，如果没有发生变化，才交换新值，发生了变化则不交换。这是很多无锁设计的基础，下一章节会详细讲述DPDK中无锁队列的设计。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在Linux内核中，提供了两组原子操作的接口：一组是针对整数的操作；一组是针对位运算的操作。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;针对整数的原子操作通常只处理atomic_t类型的数据，没有C语言中的int型，该类型可以被认为是一个24位的数据，通常用于实现计数器。而且原子整数操作往往是内联函数，通过内嵌汇编实现。另一方面如果某个函数是原子的，它也通常被定义为一个宏。&lt;/p&gt;

&lt;p&gt;原子位操作通常是实现原子地翻转、清空、设置某一地址处n位的值。&lt;/p&gt;

&lt;h3 id=&quot;内存屏障&quot;&gt;内存屏障&lt;/h3&gt;

&lt;p&gt;在处理器和编译优化经过了几十年的发展，已经为了获取更高的性能变得非常复杂，一些时候甚至编写C语言程序的程序员自己也不知道哪条语句会被先执行。那是因为在O2甚至O3的优化下，循环展开、写入折叠、乱序执行将CPU性能发挥到极致，但是也带来了一些问题，只有弱序类型的程序才可以获取这样更高的性能。&lt;strong&gt;而需要保序的程序，不能保证一些指令会在特定指令之前执行完。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了解决这一问题而引入了内存屏障的概念，虽然分为三种：读写屏障、读屏障和写屏障。但其实现上都是调用__sync_synchronize()，而内核中该函数对应着的正是MFENCE这个序列化加载和存储操作汇编指令。&lt;strong&gt;此序列化确保：在全局范围内，MFENCE前后的任何加载和存储操作以MFENCE为界限，严格保序。&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*(volatile uint16_t *)&amp;amp;vq-&amp;gt;used-&amp;gt;idx += count;
      vq-&amp;gt;last_used_idx = res_end_idx;

      rte_mb(); //DPDK封装的内存屏障，就是使用__sync_synchronize实现

      if (!(vq-&amp;gt;avail-&amp;gt;flags &amp;amp; VRING_AVAIL_F_NO_INTERRUPT))
            eventfd_write(vq-&amp;gt;callfd, (eventfd_t)1);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面代码所示是vhost-user数据包收发中一个典型应用场景，每次后端发送完了数据包，更改了avail指针，然后才可以发出eventfd通知前端接收数据包。这个顺序如果乱了，将会引发错误。&lt;/p&gt;

&lt;h1 id=&quot;3无锁队列&quot;&gt;3.无锁队列&lt;/h1&gt;

&lt;p&gt;在介绍无锁队列之前，我想先为锁正名，上面介绍到锁的提出的解决临界区的争端，会有些线程需要等待锁被释放，因此有人觉得锁是性能杀手，是锁导致了性能的下降。实际上这是不完全正确的，&lt;strong&gt;大多数时候慢的不是锁本身，而是等锁的时间&lt;/strong&gt;，这种情况下换成其他的方式也不能解决你的问题。这种情况下最佳的方式还是避免竞争，相当于说你自己的业务处理逻辑的问题，造成大量互相竞争等待甚至锁死的情况，不管是使用锁还是其他的方式，都不会提升性能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;对于无锁队列的实现，由于体系结构以及编译等问题，实际上在实现上做到没有bug是很难的。很多时候一个程序运行几十次、上百次没有任何异常，但是运行上万次可能会出现一次异常，甚至在有些场景下会产生和体系结构硬相关的异常。比方说一个我们在实际运行中遇到的bug，在核隔离情况下，一个隔离核使用CAS指令与非隔离核在缓存MESI协议中有一些同步问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面讲过的CAS操作，就是一种实现无锁队列的基础技术，这里以DPDK为例讲述其无锁队列的实现方法。以下摘自DPDK官方文档[3]：&lt;/p&gt;

&lt;p&gt;DPDK &lt;code class=&quot;highlighter-rouge&quot;&gt;rte_ring&lt;/code&gt;这一环形结构由两个头尾组成，一组由生产者使用，一组由消费者使用。下图中分别用&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_head&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_tail&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_head&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_tail&lt;/code&gt;来指代他们。&lt;strong&gt;关于该无锁队列的操作主要分为四点：单生产者入队、单消费者出队、多生产者入队和多消费者出队&lt;/strong&gt;。下面只介绍前三个，多消费者出队与前面类似。&lt;/p&gt;

&lt;h3 id=&quot;单生产者入队&quot;&gt;单生产者入队&lt;/h3&gt;

&lt;p&gt;首先将&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_tail&lt;/code&gt;复制到局部变量中。&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_next&lt;/code&gt;局部变量指向下一个对象，或者在批量入队的情况下指向下几个对象。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;中没有足够的空间（通过检查&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_tail&lt;/code&gt;检测到），则返回错误。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-59630cfe8b9a65f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单生产者入队第一步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二步是修改&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;结构中的&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;，使其指向与&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_next&lt;/code&gt;相同的位置。指向添加的对象的指针被拷贝到&lt;code class=&quot;highlighter-rouge&quot;&gt;ring(obj4)&lt;/code&gt;中。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-80854125f4080bc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单生产者入队第二步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将对象添加到&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;中后，&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_tail&lt;/code&gt;将被修改为指向与&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;相同的位置，入队操作完成。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-176c26f665c2a05b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单生产者入队第三步.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;单消费者出队&quot;&gt;单消费者出队&lt;/h3&gt;

&lt;p&gt;首先，将&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_head&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_tail&lt;/code&gt;复制到局部变量中。&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_next&lt;/code&gt;局部变量指向ring的下一个对象，或者在批量出队的情况下指向下几个对象。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;中没有足够的对象（通过检查&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_tail&lt;/code&gt;检测到这种情况），则会返回错误。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-032e6e7abbfaebf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单消费者出队第一步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二步是修改&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_head&lt;/code&gt;指向与&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_next&lt;/code&gt;相同的位置。指向出队对象&lt;code class=&quot;highlighter-rouge&quot;&gt;(obj1)&lt;/code&gt;的指针被拷贝到一个临时用户定义的指针中。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-48f5dc1db7706d29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单消费者出队第二步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后，修改&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_tail&lt;/code&gt;以指向与&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_head&lt;/code&gt;相同的位置，出列操作完成。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-8a86e99bfa56203c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;单消费者出队第三步.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;多生产者入队&quot;&gt;多生产者入队&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;这里模拟两个生产者同时进行入队操作。&lt;/em&gt;&lt;/strong&gt;
在两个core上，&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;cons_tail&lt;/code&gt;被复制到局部变量中。&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_next&lt;/code&gt;局部变量指向&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;的下一个对象，或者在批量入队的情况下指向下几个对象。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;中没有足够的空间（通过检查&lt;code class=&quot;highlighter-rouge&quot;&gt;cons_tail&lt;/code&gt;检测到），则返回错误。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-cd8516561849c0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;多生产者入队第一步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二步是修改&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;，使其指向与&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_next&lt;/code&gt;相同的位置。&lt;strong&gt;此操作使用CAS指令完成&lt;/strong&gt;，该指令以原子方式执行以下操作：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;与局部变量&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_head&lt;/code&gt;不同，则CAS操作失败，并且代码在第一步重新启动；否则，&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_head&lt;/code&gt;设置为局部变量&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_next&lt;/code&gt;，CAS操作成功，处理继续。在图中，操作在core 1上成功，在core 2上重新启动第一步。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-a993f9dd0f4037b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;多生产者入队第二步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三步，CAS操作在core 2取得成功，core 1更新&lt;code class=&quot;highlighter-rouge&quot;&gt;ring&lt;/code&gt;的一个元素&lt;code class=&quot;highlighter-rouge&quot;&gt;(obj4)&lt;/code&gt;，core 2更新另一个元素&lt;code class=&quot;highlighter-rouge&quot;&gt;(obj5)&lt;/code&gt;。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-9ed275293acb9fc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;多生产者入队第三步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第四步，每个core都想更新&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_tail&lt;/code&gt;。只有当&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_tail&lt;/code&gt;等于&lt;code class=&quot;highlighter-rouge&quot;&gt;prod_head&lt;/code&gt;局部变量，core才能更新它。这个操作只在core 1上完成。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-6b1ca9e9ad04e90b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;多生产者入队第四步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一旦&lt;code class=&quot;highlighter-rouge&quot;&gt;ring-&amp;gt;prod_tail&lt;/code&gt;在core 1上更新完成，core 2也将被允许更新它。这个操作也在core 2上完成了。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-590fd39213e7da26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;多生产者入队第五步.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;同步互斥机制的基本方法介绍完了，我们现在已有这么多种策略保证代码正确高效地运行。在选择哪种策略的时候，应当根据使用场景的具体情况而定，另外每种策略虽然内核有自己的实现，但都可以实现在用户空间高效地执行。人们对临界区同步互斥问题的研究还在继续。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;引用：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;[1] “Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors” by John M. Mellor-Crummey and Michael L. Scott. This paper received the 2006 Dijkstra Prize in Distributed Computing.&lt;/p&gt;

&lt;p&gt;[2] Linux内核RCU(Read Copy Update)锁简析, https://blog.51cto.com/dog250/1673351&lt;/p&gt;

&lt;p&gt;[3] DPDK rte_ring文档，http://doc.dpdk.org/guides/prog_guide/ring_lib.html&lt;/p&gt;

&lt;p&gt;[4]《深入浅出DPDK》&lt;/p&gt;
</description>
        <pubDate>Sun, 07 Apr 2019 01:27:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/06/lock.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/06/lock.html</guid>
        
        
        <category>operation-system</category>
        
      </item>
    
      <item>
        <title>Linux套接口缓存——sk_buff</title>
        <description>&lt;p&gt;在拥有复杂功能的现代操作中，对数据包的存储要求是非常严格的。因为其中涉及数据包在不同网络层之间传递，需要灵活增减包头，以及移动过程中尽可能避免拷贝。这在linux网络系统中，使用sk_buff数据结构来承担传递数据包的功能，也有些地方简写做SKB。&lt;/p&gt;

&lt;p&gt;SKB的操作函数和宏涉及以下文件：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inlude/linux/skbuff.h  SKB结构定义和SKB宏。
net/core/skbuff.c  操作SKB的函数。
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;SKB的数据结构定义非常长，有100多行，但这里要讲的是其中真正需要注意的有五个变量：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;unsigned char *head     用于指向数据包的开始
unsigned char *data      用于指向数据包载荷的开始
unsigned char *tail      用于指向数据包载荷的结束
unsigned char *end      用于指向数据包的结束
unsigned int len      数据包包含的数据量
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;它在实际应用中是这样发挥作用的。&lt;strong&gt;设skb指向一个sk_buff，当数据包穿过协议栈各层时，skb-&amp;gt;head，skb-&amp;gt;data，skb-&amp;gt;tail以及skb-&amp;gt;end在数据包相关缓冲区上移动&lt;/strong&gt;。如下图所示，指向正在处理数据包的协议栈头部。当一个数据包到达mac层时，skb-&amp;gt;data指向以太网帧头部，当数据包继续到达IP层时，skb-&amp;gt;data就移到IP头部的起始处。与此同时，skb-&amp;gt;len也会更新。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-549586870b79560b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;skb-&amp;gt;data在不同网络层的变化.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这样的设计就是为了便于SKB数据结构在不同层之间灵活地增删头部，另外它还是一个双向链表，便于把不同的SKB串联起来，这也是Linux网络处理的一个优化，叫做&lt;strong&gt;“聚合分散IO”&lt;/strong&gt;，也就是Linux网络处理中的零拷贝。由于有些报文发送时，会有多个分片，如果依次拷贝这些分片组装成一个单块，就会存在从用户空间多次内存拷贝到内核空间巨大的开销。因此聚合分散IO的想法就是在SKB上标注分片数目，将其他分片链接到第一个，在此过程中&lt;strong&gt;只拷贝记录分片数据位置和长度的数据缓存区到SKB中&lt;/strong&gt;，而避免多次的数据包拷贝，而后由DMA模块直接将数据从内核缓冲区传递给协议模块。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;了解了SKB在不同网络层之间的传递，那么在网卡收到数据包的时候，SKB的分配和接收数据包流程又是怎样呢。在此需要先介绍几个与SKB接收数据包相关的函数：dev_alloc_skb()、skb_reserve()、skb_put()、netif_rx(skb)。&lt;/p&gt;

&lt;p&gt;下图显示了这些函数在SKB层面上的作用。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-3fa8a43d14d7f084.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;SKB接收数据包相关函数.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实际运行中，&lt;strong&gt;前两步（dev_alloc_skb()和skb_reserve()）是在初次预分配环形接收缓冲区时执行的，第三步是由NIC硬件在将DMA接收数据存入预分配的sk_buff时完成的，最后两步（skb_put()和netif_rx()）则从接收中断服务例程开始执行&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;其中dev_alloc_skb()创建sk_buff来存放接收到的数据包，sk_buff的创建是从Slab内存分配器中直接分配的小于4K的高速缓存（Slab内存分配器以后再讲）。这是一个可在中断上下文执行的函数，它为skb_buff分配内存并将其与一个数据包载荷缓冲区关联。dev_kfree_skb()完成dev_alloc_skb()的相反功能，释放缓冲区。接下来调用的skb_reserve()在数据包缓冲的起始和载荷的开始之间增加一个2B的填充位。这使IP头能在16B边界处开始（因为对齐的原因，这通常意味着性能更佳），因为前面的以太网头部是14B。剩下的代码用收到的数据包填充载荷缓冲区，并移动skb-&amp;gt;data，skb-&amp;gt;tail以及skb-&amp;gt;len来表示这种操作。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;参考自《Linux源码剖析——TCP/IP实现》&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Apr 2019 19:35:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/04/skbuff.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/04/skbuff.html</guid>
        
        
        <category>operation-system</category>
        
      </item>
    
      <item>
        <title>神奇的fork（父子进程中一些神奇的问题）</title>
        <description>&lt;p&gt;在阅读这篇文章的时候我希望长话短说，需要你事先接收一个观点，那就是：&lt;strong&gt;我们的操作系统就是一堆进程，每一个进程都是由已有的进程创造出来的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以linux操作系统为例，在启动之后第一个诞生的进程是PID为0的名叫“idle”的进程，后续所有的进程都是由它通过fork()创建的，包括我们熟知的PID为1的“init”进程。也许一杯咖啡的时间，等系统完全启动，我们可以登录以后，在命令行执行我们自己写的程序，这又是通过终端进程创建了新的进程。这就是在前面所说的&lt;strong&gt;操作系统的运行就是进程的不断创建和销毁的过程。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在linux系统函数中，fork()是用来创建新的子进程的函数。根据linux编程手册，在fork函数执行完毕后，如果创建新进程成功，则出现两个进程，一个是子进程，一个是父进程。&lt;strong&gt;在子进程中，fork函数返回0，在父进程中，fork返回新创建子进程的进程ID&lt;/strong&gt;。我们可以通过fork返回的值来判断当前进程是子进程还是父进程。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;需要注意的是，在fork之后两个进程用的是相同的物理空间(内存区)，子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，其对应的物理空间是一个。这是出于效率的考虑，在linux中被称为“写时复制”（COW）技术，只有当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。另外fork之后内核会将子进程排在队列的前面，以让子进程先执行，以免父进程执行导致写时复制，而后子进程执行exec系统调用，因无意义的复制而造成效率的下降。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有关父子进程和fork的概念介绍完了，在这里想继续分享一道很有意思的笔试题。&lt;/p&gt;

&lt;p&gt;写出下面程序的运行结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int main()
{
       pid_t cld_pid;
       int status;
       int a=1,b=2;
       for(int i = 0; i &amp;lt; 2; i++ ){
              if ((cld_pid = fork()) == 0){
                     a+=1;
                     printf(&quot;a=%d\n&quot;,a);
              }
              else{
                     b+=1;
                     printf(&quot;b=%d\n&quot;,b);
              }
       }
       wait(&amp;amp;status);
       return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这其实就是考察了如果一个进程在for循环进行fork，会产生什么样的结果，哪些变量的值已经改变，哪些变量的值没有改变。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-0f2ac074cd5a70e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;分析&quot; /&gt;
实际上它的运行一共产生了三个子进程：&lt;/p&gt;

&lt;p&gt;首先父进程fork出了子进程1，同时自己执行了b自加和打印，得到&lt;strong&gt;“b=3”&lt;/strong&gt;。此时在第一轮循环中，子进程1的变量分别为i=0，a=1，b=2。第二轮循环中，创建了子进程2，同时执行b的自加和打印，得到&lt;strong&gt;“b=4”&lt;/strong&gt;，耗尽循环条件。而子进程2的变量为i=1，a=1，b=3。&lt;/p&gt;

&lt;p&gt;对于子进程1，会执行a的自加和打印，得到&lt;strong&gt;“a=2”&lt;/strong&gt;，完成第一轮循环。在第二轮循环中创建了子子进程1，并对b自加和打印，得到&lt;strong&gt;“b=3”&lt;/strong&gt;。子子进程1的变量分别为i=1，a=2，b=2。&lt;/p&gt;

&lt;p&gt;对于子进程2，会执行a的自加和打印，得到&lt;strong&gt;“a=2”&lt;/strong&gt;，然后由于i=1，结束。&lt;/p&gt;

&lt;p&gt;对于子子进程1，会执行a的自加和打印，得到&lt;strong&gt;“a=3”&lt;/strong&gt;，而由于i已经为1，再也没有新的循环了。&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Apr 2019 01:45:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/03/fork.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/operation-system/2019/04/03/fork.html</guid>
        
        
        <category>operation-system</category>
        
      </item>
    
      <item>
        <title>虚拟化网络中的零拷贝</title>
        <description>&lt;p&gt;数据包的拷贝是虚拟化网络中最大的性能瓶颈，这些年来，人们也一直在致力于减少内存拷贝甚至消除内存拷贝。根据数据包流向，零拷贝可以分为 Host to VM 的零拷贝和 VM to VM 的零拷贝，前者主要适用于一般的情况，后者更适用于NFV。本文简单枚举下现有的零拷贝的虚拟化网络实现。&lt;/p&gt;

&lt;h1 id=&quot;1host-to-vm零拷贝&quot;&gt;1.Host to VM零拷贝&lt;/h1&gt;
&lt;p&gt;除去之前的文章中讲过的设备透传之外，数据包到达物理网卡在转发之前都必须被存储在主机的数据包缓冲区中，这样要想实现零拷贝，必须允许虚拟机能够直接访问该处的数据包。现有实现有两种方式，一种是采用更决绝的共享内存技术，另一种是在EPT页表层面做一些动态修改。&lt;/p&gt;

&lt;h3 id=&quot;基于共享内存实现零拷贝&quot;&gt;基于共享内存实现零拷贝&lt;/h3&gt;
&lt;p&gt;早在09年就有一位博士在自己的毕业设计中提出了一种零拷贝的虚拟化网络IO接口实现，并以Nahanni命名了这种设计的设备和驱动名[1,2]，这便是IVSHMEM的前身。但是不幸的是，尽管这位博士在自己论文中以极具攻击性的语气诋毁了社区选择virtio方案的愚蠢行为，但是由于这种共享内存的设计太过大胆，存在严重的安全隐患，因而始终不被内核开发者接纳。&lt;/p&gt;

&lt;p&gt;几年以后，这项技术反而在一个新兴的方向被提出，而被更多人知晓。也许没有很多人知道IVSHMEM，但一定知道14年NSDI计算机网络顶级会议上提出的NetVM，他使用的是相同的设计思想，将之应用于NFV领域，并实现了编程所需的基本框架[3]。NetVM代表了虚拟化网络IO中零拷贝共享内存是最好实现，那么这里就用NetVM为例介绍一下这种技术如何发挥作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-f2c305ff73cc73f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;NetVM.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;他实现零拷贝的思路就是：让host的数据包buffer共享给VM，使得VM有能力直接访问主机上属于自己的数据包。另外，还需要共享一对队列，用于VM与host传输数据包过程中的交互，互相指明数据包存在哪了。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这项方式有两点缺点：由于需要VM实现对共享内存内部数据包的直接读取（无须拷贝到自己的buffer），就需要在VM中改写&lt;strong&gt;专有驱动&lt;/strong&gt;，来完成对共享内存这块buffer的内存管理；对于多个VM共享一块内存作为数据包缓冲区，意味着上面的数据包可以随意访问，存在安全性隐患。也正是这两点原因，这项方案难以大规模使用。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;但是它在NFV环境下，多VM共享一块内存的设计却是提升性能的关键。因为NFV旨在利用虚拟化的资源（如：VM或者容器）来实现物理设备实现的网络功能，从而构成服务链，也就是说一条服务链上可能有很多个运行不同网络功能的虚拟机，数据包（flow）需要穿过这么多虚拟机，内存拷贝会让性能急剧下降。因此，在NetVM方案下，一个数据包可以同时被一条服务链上所有的VM访问，结合一些NFV并行化的操作，可以大大提高NFV的性能。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面简要说明一下这种直接host与VM共享内存是如何实现的吧。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-52225e66ee27a387.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;设备共享内存.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;答案还是&lt;strong&gt;设备模拟，使用设备内存&lt;/strong&gt;。QEMU是VM主进程，也是负责给VM提供所有的设备模拟的。在启动VM的时候，QEMU给VM虚拟出来一个PCI设备，并将host的buffer作为设备内存注册进设备的数据结构。这样一旦VM需要访问设备的时候，就会被重定向到host的buffer。&lt;/p&gt;

&lt;h3 id=&quot;page-flipping实现零拷贝&quot;&gt;page-flipping实现零拷贝&lt;/h3&gt;
&lt;p&gt;这项技术在十年前，就有人提出并在Xen虚拟化机制上做了实现。最近，也有人把它实现在了QEMU/KVM虚拟化上，并且基于的是vhost-user做改动[4]。&lt;/p&gt;

&lt;p&gt;这种方法的设计思想是：&lt;strong&gt;VM内存访问是经过EPT页表，将GPA（Guest Physical Address）翻译成HPA（Host Physical Address），既然这样就可以不用共享内存，直接改数据包所在页的EPT页表表项，使得VM访存时被重定向到数据包所在的物理页。&lt;/strong&gt;
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-6a9b49ba0d0eccc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;page-flipping.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种方式就是“简单粗暴”，需要修改网卡驱动对内存的管理，以及KVM内核代码，允许EPT表项映射关系被动态修改。这也正是其缺陷所在，不论数据包大小，每次数据包传输都需要改EPT表项，而这是一个很昂贵的操作。实验结果表明，它拖累了小包的传输，&lt;strong&gt;只有在数据包大小超过1518字节（那也就是只有巨型帧了），这种方式才比数据包拷贝要快。&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;2vm-to-vm零拷贝&quot;&gt;2.VM to VM零拷贝&lt;/h1&gt;
&lt;p&gt;其实上面所说的NetVM和IVSHMEM已经实现的VM间零拷贝了，这里就说下vhost-user尚未开源的一些设计。&lt;/p&gt;

&lt;p&gt;因为virtio已经被社区接纳并且进入大部分操作系统的内核，因此对基于virtio的各项feature的优化，还是有很多人在做。我这里要说的就是virtio在VM间共享内存的技术vhost-pci。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-91388d4a3b2d6a51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;vhost-pci示意图.png&quot; /&gt;
&lt;strong&gt;它的想法非常直接，如果一个VM能够直接从另一个VM中读取数据包而不需要拷贝，那么可以极大加快服务链上的效率。这在实现上非常类似NetVM和IVSHMEM，为需要进行VM to VM传输的VM虚拟一个热插拔设备叫做vhost-pci，设备内存就是另一个VM的内存，在VM中再实现一套驱动，让VM间通信变得类似于vhost-user的前后端操作，就可以实现零拷贝。&lt;/strong&gt;在github上已经有vhost-pci的实现了：&lt;a href=&quot;https://github.com/wei-w-wang/&quot;&gt;https://github.com/wei-w-wang/&lt;/a&gt;vhost-pci，不知道为什么没有推进社区。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;关于虚拟机间通信，Intel和开源社区也还有一个更灵活的想法，让每个QEMU进程有“弱交换机”的功能，VM在往外发送数据包的时候由这个“弱交换机”判断是VM间通信还是与外部通信，从而确定走vhost-pci快速通道还是走OVS转发。而这些QEMU“弱交换机”还会和OVS进行交互更新流表项等操作。当初见到这个设计的时候真的佩服设计者的脑洞，但是现在还是停在原型验证阶段，可能里面还有什么巨坑吧。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;但是这项快速通路带来了另一个问题——安全问题。考虑到允许一个VM直接访问另一个VM内存，这是一项极其危险的操作，如果能有一个“protected view”去做这项事情就好了。&lt;/p&gt;

&lt;p&gt;为了实现这项安全地访问外部资源（敏感资源）的技术，Intel直接在硬件层面上下功夫了，并为此申请了全球专利。这项技术叫做EPTP switching。这项技术允许一个VM拥有多个EPT页表，通过写寄存器，实现EPT页表之间的切换。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;科普一下，它不仅可以用于安全地访问外部资源上，由于它的开销极低，一些虚拟机内存监视器也使用了该方法。去年“熔断”和“幽灵”两大漏洞让Linux社区为此重新设计了页表，提出的KPTI让性能降低很多；因此有人认为在云上，虚拟机可以不用打这么大开销的补丁，可以直接利用多项EPT页表技术来实现对VM内存的监视[5]。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-84f20a95c9c693a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;EPT页表切换示意图.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;回到vhost-pci策略上来，&lt;strong&gt;我们为一个VM建立两个EPT页表：一个是default页表，另一个是包含别的VM内存的EPT页表。可以调用VMFUNC中的EPTP switching在这两种页表之间切换。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-cf6bc668c9b6dce3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;数据包转发流程示意图.png&quot; /&gt;
在这种机制下的数据包传输就变成上图这样：在进入发送函数时，目的VM的内存是不可见的；先调用VMFUNC切换到protected view，目的VM的内存可见，进行数据包send；发送完以后，再调用VMFUNC切换default EPT页表，目的VM的内存又变得不可见了。&lt;/p&gt;

&lt;p&gt;这样就实现了既安全又高效的VM间数据传输，可惜这套方案已经被专利保护，再没有发挥余地了。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;引用：&lt;/em&gt;
[1] Macdonell A C. Shared-memory optimizations for virtual machines[J]. 2011.&lt;/p&gt;

&lt;p&gt;[2] MacDonell C. Nahanni, a shared memory interface for kvm[J]. Aug, 2010, 10: 19.&lt;/p&gt;

&lt;p&gt;[3] Hwang J, Ramakrishnan K K, Wood T. NetVM: High Performance and Flexible Networking Using Virtualization on Commodity Platforms[C]//11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14). 2014: 445-458.&lt;/p&gt;

&lt;p&gt;[4] Wang D, Hua B, Lu L, et al. Zcopy-vhost: Eliminating Packet Copying in Virtual Network I/O[C]//2017 IEEE 42nd Conference on Local Computer Networks (LCN). IEEE, 2017: 632-639.&lt;/p&gt;

&lt;p&gt;[5] Hua Z, Du D, Xia Y, et al. {EPTI}: Efficient Defence against Meltdown Attack for Unpatched VMs[C]//2018 {USENIX} Annual Technical Conference ({USENIX}{ATC} 18). 2018: 255-266.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;vhost-pci中图 来源于 “Extending KVM Models Toward High-Performance NFV; Jun Nakajima, James Tsai, Mesut Ergin, Yang Zhang, and Wei Wang; Intel”&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Apr 2019 19:13:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/04/02/zero-copy.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/04/02/zero-copy.html</guid>
        
        
        <category>virtualized-network-IO</category>
        
      </item>
    
      <item>
        <title>Vhost-user详解</title>
        <description>&lt;p&gt;在软件实现的网络I/O半虚拟化中，vhost-user在性能、灵活性和兼容性等方面达到了近乎完美的权衡。虽然它的提出已经过了四年多，也已经有了越来越多的新特性加入，但是万变不离其宗，那么今天就从整个vhost-user数据通路的建立过程，以及数据包传输流程等方面详细介绍下vhost-user架构，本文基于DPDK 17.11分析。&lt;/p&gt;

&lt;p&gt;vhost-user的最好实现在DPDK的vhost库里，该库包含了完整的virtio后端逻辑，可以直接在虚拟交换机中抽象成一个端口使用。在最主流的软件虚拟交换机OVS（openvswitch）中，就可以使用DPDK库。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-efe82f718952cd69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;vhost-user典型部署场景.png&quot; /&gt;
vhost-user最典型的应用场景如图所示，OVS为每个虚拟机创建一个vhost端口，实现virtio后端驱动逻辑，包括响应虚拟机收发数据包的请求，处理数据包拷贝等。每个VM实际上运行在一个独立的QEMU进程内，QEMU是负责对虚拟机设备模拟的，它已经整合了KVM通信模块，因此QEMU进程已经成为VM的主进程，其中包含vcpu等线程。QEMU启动的命令行参数可以选择网卡设备类型为virtio，它就会为每个VM虚拟出virtio设备，结合VM中使用的virtio驱动，构成了virtio的前端。&lt;/p&gt;

&lt;h1 id=&quot;1建立连接&quot;&gt;1.建立连接&lt;/h1&gt;

&lt;p&gt;前面说到VM实际上是运行在QEMU进程内的，那么VM启动的时候要想和OVS进程的vhost端口建立连接，从而实现数据包通路，就需要先建立起来一套控制信道。&lt;strong&gt;这套控制信道是基于socket进程间通信，是发生在OVS进程与QEMU进程之间，而不是与VM，另外这套通信有自己的协议标准和message的格式&lt;/strong&gt;。在DPDK的lib/librte_vhost/vhost_user.c中可以看到所有的消息类型：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static const char *vhost_message_str[VHOST_USER_MAX] = {
	[VHOST_USER_NONE] = &quot;VHOST_USER_NONE&quot;,
	[VHOST_USER_GET_FEATURES] = &quot;VHOST_USER_GET_FEATURES&quot;,
	[VHOST_USER_SET_FEATURES] = &quot;VHOST_USER_SET_FEATURES&quot;,
	[VHOST_USER_SET_OWNER] = &quot;VHOST_USER_SET_OWNER&quot;,
	[VHOST_USER_RESET_OWNER] = &quot;VHOST_USER_RESET_OWNER&quot;,
	[VHOST_USER_SET_MEM_TABLE] = &quot;VHOST_USER_SET_MEM_TABLE&quot;,
	[VHOST_USER_SET_LOG_BASE] = &quot;VHOST_USER_SET_LOG_BASE&quot;,
	[VHOST_USER_SET_LOG_FD] = &quot;VHOST_USER_SET_LOG_FD&quot;,
	[VHOST_USER_SET_VRING_NUM] = &quot;VHOST_USER_SET_VRING_NUM&quot;,
	[VHOST_USER_SET_VRING_ADDR] = &quot;VHOST_USER_SET_VRING_ADDR&quot;,
	[VHOST_USER_SET_VRING_BASE] = &quot;VHOST_USER_SET_VRING_BASE&quot;,
	[VHOST_USER_GET_VRING_BASE] = &quot;VHOST_USER_GET_VRING_BASE&quot;,
	[VHOST_USER_SET_VRING_KICK] = &quot;VHOST_USER_SET_VRING_KICK&quot;,
	[VHOST_USER_SET_VRING_CALL] = &quot;VHOST_USER_SET_VRING_CALL&quot;,
	[VHOST_USER_SET_VRING_ERR]  = &quot;VHOST_USER_SET_VRING_ERR&quot;,
	[VHOST_USER_GET_PROTOCOL_FEATURES]  = &quot;VHOST_USER_GET_PROTOCOL_FEATURES&quot;,
	[VHOST_USER_SET_PROTOCOL_FEATURES]  = &quot;VHOST_USER_SET_PROTOCOL_FEATURES&quot;,
	[VHOST_USER_GET_QUEUE_NUM]  = &quot;VHOST_USER_GET_QUEUE_NUM&quot;,
	[VHOST_USER_SET_VRING_ENABLE]  = &quot;VHOST_USER_SET_VRING_ENABLE&quot;,
	[VHOST_USER_SEND_RARP]  = &quot;VHOST_USER_SEND_RARP&quot;,
	[VHOST_USER_NET_SET_MTU]  = &quot;VHOST_USER_NET_SET_MTU&quot;,
	[VHOST_USER_SET_SLAVE_REQ_FD]  = &quot;VHOST_USER_SET_SLAVE_REQ_FD&quot;,
	[VHOST_USER_IOTLB_MSG]  = &quot;VHOST_USER_IOTLB_MSG&quot;,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;随着版本迭代，越来越多的feature被加进来，消息类型也越来越多，但是&lt;strong&gt;控制信道最主要的功能就是：传递建立数据通路必须的数据结构；控制数据通路的开启和关闭以及重连功能；热迁移或关闭虚拟机时传递断开连接的消息。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从虚拟机启动到数据通路建立完毕，所传递的消息都会记录在OVS日志文件中，对这些消息整理过后，实际流程如下图所示：
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-e1cd86d0c6f15c16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;vhost-user控制信道消息流.png&quot; /&gt;
左边一半为一些协商特性，尤其像后端驱动与前端驱动互相都不知道对方协议版本的时候，协商这些特性是必要的。特性协商完毕，接下来就要传递建立数据通路所必须的数据结构，主要包括传递共享内存的文件描述符和内存地址的转换关系，以及virtio中虚拟队列的状态信息。下面对这些最关键的部分一一详细解读。&lt;/p&gt;

&lt;h3 id=&quot;设置共享内存&quot;&gt;设置共享内存&lt;/h3&gt;

&lt;p&gt;在虚拟机中，内存是由QEMU进程提前分配好的。QEMU一旦选择了使用vhost-user的方式进行网络通信，就需要配置VM的内存访问方式为共享的，具体的命令行参数在DPDK的文档中也有说明：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-object memory-backend-file,share=on,...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这意味着&lt;strong&gt;虚拟机的内存必须是预先分配好的大页面且允许共享给其他进程&lt;/strong&gt;，具体原因在前一篇文章讲过，因为OVS和QEMU都是用户态进程，而数据包拷贝过程中，需要OVS进程里拥有访问QEMU进程里的虚拟机buffer的能力，所以VM内存必须被共享给OVS进程。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;一些节约内存的方案，像virtio_balloon这样动态改变VM内存的方法不再可用，原因也很简单，OVS不可能一直跟着虚拟机不断改变共享内存啊，这样多费事。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;而在vhost库中，后端驱动接收控制信道来的消息，主动映射VM内存的代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int vhost_user_set_mem_table(struct virtio_net *dev, struct VhostUserMsg *pmsg)
{
   ...
   mmap_size = RTE_ALIGN_CEIL(mmap_size, alignment);

   mmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE,
				 MAP_SHARED | MAP_POPULATE, fd, 0);
   ...
   reg-&amp;gt;mmap_addr = mmap_addr;
   reg-&amp;gt;mmap_size = mmap_size;
   reg-&amp;gt;host_user_addr = (uint64_t)(uintptr_t)mmap_addr +
				      mmap_offset;
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;它使用的linux库函数&lt;code class=&quot;highlighter-rouge&quot;&gt;mmap()&lt;/code&gt;来映射VM内存，详见linux编程手册http://man7.org/linux/man-pages/man2/mmap.2.html。注意到在映射内存之前它还干了一件事，设置内存对齐，这是因为mmap函数映射内存的基本单位是一个页，也就是说起始地址和大小都必须是页大小是整数倍，在大页面环境下，就是2MB或者1GB。只有对齐以后才能保证映射共享内存不出错，以及后续访存行为不会越界。&lt;/p&gt;

&lt;p&gt;后面三行是保存地址转换关系的信息。这里涉及到几种地址转换，在vhost-user中最复杂的就是地址转换。&lt;strong&gt;从QEMU进程角度看虚拟机内存有两种地址：GPA（Guest Physical Address）和QVA（QEMU Virtual Address）；从OVS进程看虚拟机内存也有两种地址GPA（Guest Physical Address）和VVA（vSwitch Virtual Address）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GPA是virtio最重要的地址，在virtio的标准中，用来存储数据包地址的虚拟队列virtqueue里面每项都是以GPA表示的。&lt;/strong&gt;但是对于操作系统而言，我们在进程内访问实际使用的都是虚拟地址，物理地址已经被屏蔽，也就是说进程只有拿到了物理地址所对应的虚拟地址才能够去访存（&lt;em&gt;我们编程使用的指针都是虚拟地址&lt;/em&gt;）。&lt;/p&gt;

&lt;p&gt;QEMU进程容易实现，毕竟作为VM主进程给VM预分配内存时就建立了QVA到GPA的映射关系。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-9dcd919eb2a26196.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;共享内存映射关系.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于OVS进程，以上图为例，&lt;strong&gt;mmap()函数返回的也是虚拟地址，是VM内存映射到OVS地址空间的起始地址（就是说我把这一块内存映射在了以mmap_addr起始的大小为1GB的空间里）。这样给OVS进程一个GPA，OVS进程可以利用地址偏移算出对应的VVA，然后实施访存。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;但是实际映射内存比图例复杂的多，可能VM内存被拆分成了几块，有些块起始的GPA不为0，这就需要在映射块中再加一个GPA偏移量，才能完整保留下来VVA与GPA之间的地址对应关系。这些对应关系是后续数据通路实现的基础。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;还有一点技术细节值得注意的是，在socket中，一般是不可以直接传递文件描述符的，文件描述符在编程角度就是一个int型变量，直接传过去就是一个整数，这里也是做了一点技术上的trick，感兴趣的话也可以研究下&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;设置虚拟队列信息&quot;&gt;设置虚拟队列信息&lt;/h3&gt;

&lt;p&gt;虚拟队列的结构由三部分构成：avail ring、used ring和desc ring。这里稍微详细说明一下这三个ring的作用与设计思想。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;传统网卡设计中只有一个环表，但是有两个指针，分别为驱动和网卡设备管理的。这就是一个典型的生产者-消费者问题，生产数据包的一方移动指针，另一方追逐，直到全部消费完。但是这么做的缺点在于，只能顺序执行，前一个描述符处理完之前，后一个只能等待。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;但在虚拟队列中，把生产者和消费者分离开来。desc ring中存放的是真实的数据包描述符（就是数据包或其缓冲区地址），avail ring和used ring中存放的指向desc ring中项的索引。&lt;/strong&gt;前端驱动将生产出来的描述符放到avail ring中，后端驱动把已经消费的描述符放到used ring中（其实就是写desc ring中的索引，即序号）。这样前端驱动就可以根据used ring来回收已经使用的描述符，即使中间有描述符被占用，也不会影响被占用描述符之后的描述符的回收。另外DPDK还针对这种结构做了一种cache层面上预取的优化，使之更加高效。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在控制信道建立完共享内存以后，还需要在后端也建立与前端一样的虚拟队列数据结构。所需要的信息主要有：desc ring的项数（不同的前端驱动不同，比如DPDK virtio驱动和内核virtio驱动就不一样）、上次avail ring用到哪（这主要是针对重连或动态迁移的，第一次建立连接此项应为0）、虚拟队列三个ring的起始地址、设置通知信号。&lt;/p&gt;

&lt;p&gt;这几项消息处理完以后，&lt;strong&gt;后端驱动利用接收到的起始地址创建了一个和前端驱动一模一样的虚拟队列数据结构，并已经准备好收发数据包。&lt;/strong&gt;其中最后两项eventfd是用于需要通知的场景，例如：虚拟机使用内核virtio驱动，每次OVS的vhost端口往虚拟机发送数据包完成，都需要使用eventfd通知内核驱动去接收该数据包，在轮询驱动下，这些eventfd就没有意义了。&lt;/p&gt;

&lt;p&gt;另外，&lt;code class=&quot;highlighter-rouge&quot;&gt;VHOST_USER_GET_VRING_BASE&lt;/code&gt;是一个非常奇特的信号，只在虚拟机关机或者断开时会由QEMU发送给OVS进程，意味着断开数据通路。&lt;/p&gt;

&lt;h1 id=&quot;2数据通路处理&quot;&gt;2.数据通路处理&lt;/h1&gt;

&lt;p&gt;数据通路的实现在DPDK的lib/librte_vhost/virtio_net.c中，虽然代码看起来非常冗长，但是其中大部分都是处理各种特性以及硬件卸载功能的，主要逻辑却非常简单。&lt;/p&gt;

&lt;p&gt;负责数据包的收发的主函数为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;uint16_t rte_vhost_enqueue_burst(int vid, uint16_t queue_id, struct rte_mbuf **pkts, uint16_t count)
//数据包流向 OVS 到 VM
uint16_t rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
	struct rte_mempool *mbuf_pool, struct rte_mbuf **pkts, uint16_t count)
//数据包流向 VM 到 OVS
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;具体的发送过程概括来说就是，&lt;strong&gt;如果OVS往VM发送数据包，对应的vhost端口去avail ring中读取可用的buffer地址，转换成VVA后，进行数据包拷贝，拷贝完成后发送eventfd通知VM；如果VM往OVS发送，则相反，从VM内的数据包缓冲区拷贝到DPDK的mbuf数据结构。&lt;/strong&gt;以下贴一段代码注释吧，不要管里面的iommu、iova，那些都是vhost-user的新特性，可用理解为iova就是GPA。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;uint16_t
rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
	struct rte_mempool *mbuf_pool, struct rte_mbuf **pkts, uint16_t count)
{
	struct virtio_net *dev;
	struct rte_mbuf *rarp_mbuf = NULL;
	struct vhost_virtqueue *vq;
	uint32_t desc_indexes[MAX_PKT_BURST];
	uint32_t used_idx;
	uint32_t i = 0;
	uint16_t free_entries;
	uint16_t avail_idx;

	dev = get_device(vid);   //根据vid获取dev实例
	if (!dev)
		return 0;

	if (unlikely(!is_valid_virt_queue_idx(queue_id, 1, dev-&amp;gt;nr_vring))) {
		RTE_LOG(ERR, VHOST_DATA, &quot;(%d) %s: invalid virtqueue idx %d.\n&quot;,
			dev-&amp;gt;vid, __func__, queue_id);
		return 0;
	}          //检查虚拟队列id是否合法

	vq = dev-&amp;gt;virtqueue[queue_id];      //获取该虚拟队列

	if (unlikely(rte_spinlock_trylock(&amp;amp;vq-&amp;gt;access_lock) == 0))	//对该虚拟队列加锁
		return 0;

	if (unlikely(vq-&amp;gt;enabled == 0))		//如果vq不可访问，对虚拟队列解锁退出
		goto out_access_unlock;

	vq-&amp;gt;batch_copy_nb_elems = 0;  //批处理需要拷贝的数据包数目

	if (dev-&amp;gt;features &amp;amp; (1ULL &amp;lt;&amp;lt; VIRTIO_F_IOMMU_PLATFORM))
		vhost_user_iotlb_rd_lock(vq);

	if (unlikely(vq-&amp;gt;access_ok == 0))
		if (unlikely(vring_translate(dev, vq) &amp;lt; 0))  //因为IOMMU导致的，要翻译iova_to_vva
			goto out;

	if (unlikely(dev-&amp;gt;dequeue_zero_copy)) {   //零拷贝dequeue
		struct zcopy_mbuf *zmbuf, *next;
		int nr_updated = 0;

		for (zmbuf = TAILQ_FIRST(&amp;amp;vq-&amp;gt;zmbuf_list);
		     zmbuf != NULL; zmbuf = next) {
			next = TAILQ_NEXT(zmbuf, next);

			if (mbuf_is_consumed(zmbuf-&amp;gt;mbuf)) {
				used_idx = vq-&amp;gt;last_used_idx++ &amp;amp; (vq-&amp;gt;size - 1);
				update_used_ring(dev, vq, used_idx,
						 zmbuf-&amp;gt;desc_idx);
				nr_updated += 1;

				TAILQ_REMOVE(&amp;amp;vq-&amp;gt;zmbuf_list, zmbuf, next);
				restore_mbuf(zmbuf-&amp;gt;mbuf);
				rte_pktmbuf_free(zmbuf-&amp;gt;mbuf);
				put_zmbuf(zmbuf);
				vq-&amp;gt;nr_zmbuf -= 1;
			}
		}

		update_used_idx(dev, vq, nr_updated);
	}

	/*
	 * Construct a RARP broadcast packet, and inject it to the &quot;pkts&quot;
	 * array, to looks like that guest actually send such packet.
	 *
	 * Check user_send_rarp() for more information.
	 *
	 * broadcast_rarp shares a cacheline in the virtio_net structure
	 * with some fields that are accessed during enqueue and
	 * rte_atomic16_cmpset() causes a write if using cmpxchg. This could
	 * result in false sharing between enqueue and dequeue.
	 *
	 * Prevent unnecessary false sharing by reading broadcast_rarp first
	 * and only performing cmpset if the read indicates it is likely to
	 * be set.
	 */
	//构造arp包注入到mbuf（pkt数组），看起来像是虚拟机发送的
	if (unlikely(rte_atomic16_read(&amp;amp;dev-&amp;gt;broadcast_rarp) &amp;amp;&amp;amp;
			rte_atomic16_cmpset((volatile uint16_t *)
				&amp;amp;dev-&amp;gt;broadcast_rarp.cnt, 1, 0))) {

		rarp_mbuf = rte_pktmbuf_alloc(mbuf_pool);    //从mempool中分配一个mbuf给arp包
		if (rarp_mbuf == NULL) {
			RTE_LOG(ERR, VHOST_DATA,
				&quot;Failed to allocate memory for mbuf.\n&quot;);
			return 0;
		}
		//构造arp报文，构造成功返回0
		if (make_rarp_packet(rarp_mbuf, &amp;amp;dev-&amp;gt;mac)) {
			rte_pktmbuf_free(rarp_mbuf);
			rarp_mbuf = NULL;
		} else {
			count -= 1;
		}
	}
	//计算有多少数据包，现在的avail的索引减去上次停止时的索引值，若没有直接释放vq锁退出
	free_entries = *((volatile uint16_t *)&amp;amp;vq-&amp;gt;avail-&amp;gt;idx) -
			vq-&amp;gt;last_avail_idx;
	if (free_entries == 0)
		goto out;

	LOG_DEBUG(VHOST_DATA, &quot;(%d) %s\n&quot;, dev-&amp;gt;vid, __func__);

	/* Prefetch available and used ring */
	//预取前一次used和avail ring停止位置索引
	avail_idx = vq-&amp;gt;last_avail_idx &amp;amp; (vq-&amp;gt;size - 1);
	used_idx  = vq-&amp;gt;last_used_idx  &amp;amp; (vq-&amp;gt;size - 1);
	rte_prefetch0(&amp;amp;vq-&amp;gt;avail-&amp;gt;ring[avail_idx]);
	rte_prefetch0(&amp;amp;vq-&amp;gt;used-&amp;gt;ring[used_idx]);

	//此次接收过程的数据包数目，为待处理数据包总数、批处理数目最小值
	count = RTE_MIN(count, MAX_PKT_BURST);
	count = RTE_MIN(count, free_entries);
	LOG_DEBUG(VHOST_DATA, &quot;(%d) about to dequeue %u buffers\n&quot;,
			dev-&amp;gt;vid, count);

	/* Retrieve all of the head indexes first to avoid caching issues. */
	//从avail ring中取得所有数据包在desc中的索引，存在局部变量desc_indexes数组中
	for (i = 0; i &amp;lt; count; i++) {
		avail_idx = (vq-&amp;gt;last_avail_idx + i) &amp;amp; (vq-&amp;gt;size - 1);
		used_idx  = (vq-&amp;gt;last_used_idx  + i) &amp;amp; (vq-&amp;gt;size - 1);
		desc_indexes[i] = vq-&amp;gt;avail-&amp;gt;ring[avail_idx];

		if (likely(dev-&amp;gt;dequeue_zero_copy == 0))	//若不支持dequeue零拷贝的话，直接将索引写入used ring中
			update_used_ring(dev, vq, used_idx, desc_indexes[i]);
	}

	/* Prefetch descriptor index. */
	rte_prefetch0(&amp;amp;vq-&amp;gt;desc[desc_indexes[0]]);	//从desc中预取第一个要发的数据包描述符
	for (i = 0; i &amp;lt; count; i++) {
		struct vring_desc *desc, *idesc = NULL;
		uint16_t sz, idx;
		uint64_t dlen;
		int err;

		if (likely(i + 1 &amp;lt; count))
			rte_prefetch0(&amp;amp;vq-&amp;gt;desc[desc_indexes[i + 1]]); //预取后一项

		if (vq-&amp;gt;desc[desc_indexes[i]].flags &amp;amp; VRING_DESC_F_INDIRECT) {  //如果该项支持indirect desc，按照indirect处理
			dlen = vq-&amp;gt;desc[desc_indexes[i]].len;
			desc = (struct vring_desc *)(uintptr_t)     //地址转换成vva
				vhost_iova_to_vva(dev, vq,
						vq-&amp;gt;desc[desc_indexes[i]].addr,
						&amp;amp;dlen,
						VHOST_ACCESS_RO);
			if (unlikely(!desc))
				break;

			if (unlikely(dlen &amp;lt; vq-&amp;gt;desc[desc_indexes[i]].len)) {
				/*
				 * The indirect desc table is not contiguous
				 * in process VA space, we have to copy it.
				 */
				idesc = alloc_copy_ind_table(dev, vq,
						&amp;amp;vq-&amp;gt;desc[desc_indexes[i]]);
				if (unlikely(!idesc))
					break;

				desc = idesc;
			}

			rte_prefetch0(desc);   //预取数据包
			sz = vq-&amp;gt;desc[desc_indexes[i]].len / sizeof(*desc);
			idx = 0;
		} 
		else {
			desc = vq-&amp;gt;desc;    //desc数组
			sz = vq-&amp;gt;size;		//size，个数
			idx = desc_indexes[i];	//desc索引项
		}

		pkts[i] = rte_pktmbuf_alloc(mbuf_pool);   //给正在处理的这个数据包分配mbuf
		if (unlikely(pkts[i] == NULL)) {	//分配mbuf失败，跳出包处理
			RTE_LOG(ERR, VHOST_DATA,
				&quot;Failed to allocate memory for mbuf.\n&quot;);
			free_ind_table(idesc);
			break;
		}

		err = copy_desc_to_mbuf(dev, vq, desc, sz, pkts[i], idx,
					mbuf_pool);
		if (unlikely(err)) {
			rte_pktmbuf_free(pkts[i]);
			free_ind_table(idesc);
			break;
		}

		if (unlikely(dev-&amp;gt;dequeue_zero_copy)) {
			struct zcopy_mbuf *zmbuf;

			zmbuf = get_zmbuf(vq);
			if (!zmbuf) {
				rte_pktmbuf_free(pkts[i]);
				free_ind_table(idesc);
				break;
			}
			zmbuf-&amp;gt;mbuf = pkts[i];
			zmbuf-&amp;gt;desc_idx = desc_indexes[i];

			/*
			 * Pin lock the mbuf; we will check later to see
			 * whether the mbuf is freed (when we are the last
			 * user) or not. If that's the case, we then could
			 * update the used ring safely.
			 */
			rte_mbuf_refcnt_update(pkts[i], 1);

			vq-&amp;gt;nr_zmbuf += 1;
			TAILQ_INSERT_TAIL(&amp;amp;vq-&amp;gt;zmbuf_list, zmbuf, next);
		}

		if (unlikely(!!idesc))
			free_ind_table(idesc);
	}
	vq-&amp;gt;last_avail_idx += i;

	if (likely(dev-&amp;gt;dequeue_zero_copy == 0)) {  //实际此次批处理的所有数据包内容拷贝
		do_data_copy_dequeue(vq);
		vq-&amp;gt;last_used_idx += i;
		update_used_idx(dev, vq, i);  //更新used ring的当前索引，并eventfd通知虚拟机接收完成
	}

out:
	if (dev-&amp;gt;features &amp;amp; (1ULL &amp;lt;&amp;lt; VIRTIO_F_IOMMU_PLATFORM))
		vhost_user_iotlb_rd_unlock(vq);

out_access_unlock:
	rte_spinlock_unlock(&amp;amp;vq-&amp;gt;access_lock);

	if (unlikely(rarp_mbuf != NULL)) {	//再次检查有arp报文需要发送的话，就加入到pkts数组首位，虚拟交换机的mac学习表就能第一时间更新
		/*
		 * Inject it to the head of &quot;pkts&quot; array, so that switch's mac
		 * learning table will get updated first.
		 */
		memmove(&amp;amp;pkts[1], pkts, i * sizeof(struct rte_mbuf *));
		pkts[0] = rarp_mbuf;
		i += 1;
	}

	return i;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;在软件实现的网络功能中大量使用批处理，而且一批的数目是可以自己设置的，但是一般约定俗成批处理最多32个数据包。&lt;/p&gt;

&lt;h1 id=&quot;3ovs轮询逻辑&quot;&gt;3.OVS轮询逻辑&lt;/h1&gt;

&lt;p&gt;在OVS中有很多这样的vhost端口，DPDK加速的OVS已经实现绑核轮询这些端口了。因此一般会把众多的端口尽可能均匀地绑定到有限的CPU核上，一些厂商在实际的生产环境中如下图所示，甚至做到了以队列为单位来负载均衡。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-9bb01b7dca8571e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;OVS多队列负载均衡.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另外针对物理端口和虚拟端口的绑核也有一些优化问题，主要是为了避免读写锁，比如：把物理网卡的端口全部绑定到一个核上，软件的虚拟端口放到另一个核上，这样收发很少在一个核上碰撞等。&lt;/p&gt;

&lt;p&gt;OVS每个轮询核的工作也非常简单，&lt;strong&gt;对于每个轮询到的端口，查看它有多少数据包需要接收，接收完这些数据包后，对它们进行查表（flow table，主要是五元组匹配，找到目的端口），然后依次调用对应端口的发送函数发送出去（就像vhost端口的rte_vhost_enqueue_burst函数），全部完成后继续轮询下一个端口。&lt;/strong&gt;而SLA、Qos机制通常都是在调用对应端口的发送函数之前起作用，比如：查看该端口的令牌桶是否有足够令牌发送这些数据包，如果需要限速会选择性丢掉一些数据包，然后执行发送函数。&lt;/p&gt;

&lt;p&gt;其实不管是哪种软件实现的转发逻辑，都始终遵循在数据通路上尽可能简单，其他的机制、消息响应可以复杂多样的哲学。&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Apr 2019 01:33:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/04/01/vhost-user.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/04/01/vhost-user.html</guid>
        
        
        <category>virtualized-network-IO</category>
        
      </item>
    
      <item>
        <title>浅谈网络I/O全虚拟化、半虚拟化和I/O透传</title>
        <description>&lt;p&gt;众所周知，虚拟化技术旨在将有限的物理资源（CPU、内存等）抽象成更多份的虚拟资源供上层应用使用。最常用的领域有云服务提供商，SDN/NFV，比如：在一台物理服务器上运行成百上千的虚拟机，并将单个虚拟机出租给不同用户。&lt;/p&gt;

&lt;p&gt;虚拟化的实现主要由三项关键技术构成：CPU虚拟化、内存虚拟化和I/O虚拟化。其中CPU和内存虚拟化多由硬件支持实现，如intel VT-x、VT-d等IA架构扩展，还有EPT页表等。而I/O虚拟化则没有CPU和内存虚拟化那么统一，由于其大量基于软件实现，因此发展过程中，衍生出了好多种性能、灵活性各异的方案。我们通常所说的全虚拟化和半虚拟化一般都是在网络I/O虚拟化中体现的差异最大。&lt;/p&gt;

&lt;h1 id=&quot;1网络io全虚拟化&quot;&gt;1.网络I/O全虚拟化&lt;/h1&gt;

&lt;p&gt;该方式采用&lt;strong&gt;软件模拟真实硬件设备&lt;/strong&gt;。一个设备的所有功能或者总线结构（如设备枚举、识别、中断和DMA）等都可以在宿主机中模拟。而对客户机而言看到的是一个功能齐全的“真实”的硬件设备。其实现上通常需要宿主机上的软件配合截取客户机对I/O设备的各种请求，通过软件去模拟。比如：QEMU/KVM虚拟化中QEMU就可以通过模拟各种类型的网卡。&lt;/p&gt;

&lt;p&gt;这种方式的好处是&lt;strong&gt;灵活，不需要专有驱动&lt;/strong&gt;，因此能实现既不需要修改客户机、也无需考虑底层硬件，对客户机透明地使用网络资源。但是缺点在于，很多中断信号的处理需要让客户机感觉到自己运行在一个“真实环境”，因此QEMU软件模拟的很底层，&lt;strong&gt;效率低下&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;2网络io半虚拟化&quot;&gt;2.网络I/O半虚拟化&lt;/h1&gt;

&lt;p&gt;在这种虚拟化中，客户机操作系统能够感知到自己是虚拟机，I/O的虚拟化由前端驱动和后端驱动共同模拟实现。&lt;strong&gt;在客户机中运行的驱动程序称之为前端，在宿主机上与前端通信的驱动程序称之为后端。前端发送客户机请求给后端，后端驱动处理完这些请求后再返回给前端。&lt;/strong&gt;而在不同的虚拟化机制中，这一过程的实现手段也有所区别，例如：Xen中的共享内存、授权表，KVM中的virtio。它相比于全虚拟化的好处在于，不再需要宿主机上专门的软件去模拟实现I/O请求，因此不会有VM-exit，性能会有所提升。&lt;/p&gt;

&lt;p&gt;半虚拟化中的virtio是IBM于2005年提出的一套方案[1]，经过了十多年的发展，其驱动现在基本已经被主流的操作系统接纳编入内核，因此virtio也已经成为半虚拟化的一套事实标准。其主要结构如下图所示，前后端驱动通过虚拟队列通信，虚拟队列中又包含used ring、avail ring和desc ring。其具体接口标准参见IBM网站https://www.ibm.com/developerworks/cn/linux/1402_caobb_virtio/
，这里不做过多介绍。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-a6266c43db31ad20.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图1 virtio环表.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以virtio为标准的半虚拟化在其追寻性能的道路上也历经了三个演进方案：virtio-net、vhost-net和vhost-user。&lt;/p&gt;

&lt;h3 id=&quot;virtio-net&quot;&gt;virtio-net&lt;/h3&gt;

&lt;p&gt;如下图所示，KVM负责为程序提供虚拟化硬件的内核模块，QEMU利用KVM模拟VM运行环境，包括处理器和外设等；Tap是内核中的虚拟以太网设备，可以理解为内核bridge。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-7f218e7e2c32d6c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图2 virtio-net.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当客户机发送报文时，它会利用消息通知机制通知KVM，并退出到用户空间的QEMU进程，然后由QEMU对Tap设备进行读写（需要说明的是，QEMU是VM运行的主进程，因此才有退出这一说）。
在该模型中，&lt;strong&gt;宿主机、客户机和QEMU存在大量的上下文切换，以及频繁的数据拷贝、CPU特权级切换&lt;/strong&gt;，因此性能差强人意。其函数调用路径如下：
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-d6aa037a5d2ae318.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图3 virtio-net数据包处理调用流程.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;两次报文拷贝&lt;/strong&gt;导致性能瓶颈，另外消息机制处理过程太长：报文到达Tap时内核通知QEMU，QEMU利用IOCTL向KVM请求中断，KVM发送中断到客户机。&lt;/p&gt;

&lt;h3 id=&quot;vhost-net&quot;&gt;vhost-net&lt;/h3&gt;

&lt;p&gt;针对virtio-net的优化是把QEMU从消息队列的处理中解放出来，直接在宿主机实现了一个vhost-net内核模块，专门做virtio的后端，以此减少上下文切换和数据包拷贝。&lt;/p&gt;

&lt;p&gt;其结构如下图所示，以报文接收过程为例。数据通路直接从Tap设备接收数据报文，通过vhost-net内核模块把报文拷贝到虚拟队列中的数据区，从而使客户机接收报文。消息通路是当报文从Tap设备到达vhost-net时，通过KVM向客户机发送中断，通知客户机接收报文。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-60b6cfa5fbab7176.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图4 vhost-net.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在数据通路层面，vhost-net减少了内存拷贝，但是由于其后端运行在内核态，仍然存在性能瓶颈。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;vhost-user&quot;&gt;vhost-user&lt;/h3&gt;

&lt;p&gt;vhost-user是采用DPDK用户态后端实现的高性能半虚拟化网络I/O。其实现机理与vhost-net类似，但是整个后端包括ovs（openvswitch） datapath全部置于用户空间，更好的利用DPDK加速。&lt;/p&gt;

&lt;p&gt;然而由于OVS进程是用户态进程，无权限访问客户机内存，因此需要使用共享内存技术，提前通过socket通信在客户机启动时，告知OVS自己的内存布局和virtio中虚拟队列信息等。这样OVS建立起对每个VM的共享内存，便可以在用户态实现上述vhost-net内核模块的功能。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-fc6183d66f87c38c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图5 vhost-user.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;vdpa加速的vhost-user&quot;&gt;vDPA加速的vhost-user&lt;/h3&gt;

&lt;p&gt;在DPDK加速的vhost-user方案中，还有一次内存拷贝。半虚拟化中仅剩的性能瓶颈也就在这一次拷贝中，intel推出了一款硬件解决方案，直接让网卡与客户机内的virtio虚拟队列交互，把数据包DMA到客户机buffer内，在支持了virtio标准的基础上实现了真正意义上的&lt;strong&gt;零拷贝&lt;/strong&gt;。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-92b0ecd9739a0e6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图6 vDPA.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在18.05以后的DPDK版本中，已经有支持vDPA的feature供选择了。&lt;/p&gt;

&lt;h1 id=&quot;3io透传&quot;&gt;3.I/O透传&lt;/h1&gt;

&lt;p&gt;对于性能的追求是永无止境的，除了上述全虚拟化、半虚拟化两种I/O虚拟化以外，还有一种非常极端的做法。让物理设备穿过宿主机、虚拟化层，直接被客户机使用，这种方式通常可以获取近乎native的性能。&lt;/p&gt;

&lt;p&gt;这种方式主要缺点是：
&lt;strong&gt;&lt;em&gt;1.硬件资源昂贵且有限。&lt;/em&gt;&lt;/strong&gt;
&lt;strong&gt;&lt;em&gt;2.动态迁移问题，宿主机并不知道设备的运行的内部状态，状态无法迁移或恢复。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DPDK针对这两点问题都做了一定程度的解决。另外还提供了一种基于硬件的PF（物理功能）转VF（虚拟功能），这相当于在网卡层面上就已经有了虚拟化的概念，把一个网卡的PF虚拟成几十上百个VF，这样可以把不同的VF透传给不同的虚拟机，这就是我们最熟悉的SR-IOV。&lt;/p&gt;

&lt;p&gt;对于I/O透传在虚拟化环境中最严重的问题不是性能了，而是灵活性。客户机和网卡之间没有任何软件中间层过度，也就意味着不存在负责交换转发功能的I/O栈，也就不会有软件交换机。那么如果要想有一台server内部的软件交换功能如何实现呢。业界的主要做法是把交换功能完全下沉到网卡，直接在智能网卡上实现虚拟交换功能。这又带来了另一个问题，成本和性能的权衡。
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/5971286-2c42750408b96651.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图7 SR-IOV.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而DPDK 18.05以后的版本似乎也解决了这一灵活性问题，为了充分发掘标准网卡（区别于智能网卡）在flow（流）层面上的功能，推出了VF representer。可以直接将OVS上的流表规则下发到网卡上，实现网卡在VF之间的交换功能，这样就实现了高效灵活的虚拟化网络配置。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;引用：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;全虚拟化、半虚拟化和I/O透传主体观点来自《深入浅出DPDK》&lt;/p&gt;

&lt;p&gt;[1] Russell R . virtio : Towards a De-Facto Standard For Virtual I/O Devices[M]. ACM, 2008.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图引用：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;图1 引用自 IBM developer &lt;a href=&quot;https://www.ibm.com/developerworks/cn/linux/1402_caobb_virtio/&quot;&gt;https://www.ibm.com/developerworks/cn/linux/1402_caobb_virtio/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;图2、4、5 引用自 “Accelerating the Path to the Guest - Maryam Tahhan and Kevin Traynor, Intel”&lt;/p&gt;

&lt;p&gt;图6 引用自 KVM forum 2017 “virtio: vhost Data Path Acceleration towards NFV Cloud - Cunming Liang, Intel”&lt;/p&gt;

&lt;p&gt;图7 引用自 DPDK官网文档
 &lt;a href=&quot;https://doc.dpdk.org/guides/prog_guide/switch_representation.html?highlight=representer&quot;&gt;https://doc.dpdk.org/guides/prog_guide/switch_representation.html?highlight=representer&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Apr 2019 00:53:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/03/31/virtualized-IO.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2019/03/31/virtualized-IO.html</guid>
        
        
        <category>virtualized-network-IO</category>
        
      </item>
    
      <item>
        <title>DPDK简单example的阅读——l2fwd</title>
        <description>&lt;p&gt;从大四开始嚷嚷着要学DPDK，一直没有静下心来看源码，拖了两年到现在才开始钻研DPDK的简单应用。二层转发是DPDK数据报处理应用里一个比较简单的example，代码只有几百行，全部看懂也大约只要半天时间。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在计算机网络中，二层是链路层，是以太网所在的层，识别的是设备端口的MAC地址。DPDK作为用户态驱动，主要的目的也就是不需要让报文经过操作系统协议栈而能实现快速的转发功能。网卡驱动在二层上的作用就是根据设定的目的端口，转发报文到目的端口。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;l2fwd的运行效果如下：&lt;/strong&gt;
将两台机器用网线相连，一台用pktgen发送数据，一台用l2fwd转发数据，l2fwd的运行界面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-e40a160b75613906.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;l2fwd运行界面.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于只开了一个端口转发，所以在l2fwd的默认规则下，就是单个port自己收自己发，发送的报文数量和接收的一样多。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;程序的主要流程如下：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-43474dabaf94f681.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;l2fwd主流程图.png&quot; /&gt;
&lt;strong&gt;每个逻辑核在任务分发后会执行如下的循环，直到退出：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-be99bf9e599f6123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;从线程循环.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中打印时间片在命令行参数中是可以自己设置的。&lt;/p&gt;

&lt;h2 id=&quot;1解析命令行参数&quot;&gt;1.解析命令行参数&lt;/h2&gt;
&lt;p&gt;DPDK的命令行参数包括：EAL参数和程序自身的参数，之间用“–”隔开。比如说，运行l2fwd时，输入命令 
&lt;code class=&quot;highlighter-rouge&quot;&gt;./l2fwd -c 0x3 -n 4 -- -p 3 -q 1&lt;/code&gt;
其中-c和-n就是EAL参数，后面的-p和-q就是程序自带的参数
所以在代码中，解析命令行参数，也分了两步，先解析的是EAL参数&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ret = rte_eal_init(argc, argv);
	if (ret &amp;lt; 0)
		rte_exit(EXIT_FAILURE, &quot;Invalid EAL arguments\n&quot;);
	argc -= ret;
	argv += ret;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;rte_eal_init不仅有解析命令行参数的作用，以及一系列很复杂的环境的初始化，详见前一篇。当解析完了EAL的参数之后，argc减去EAL参数的个数同时argv后移这么多位，这样就能保证后面解析程序参数的时候跳过了前面的EAL参数。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ret = l2fwd_parse_args(argc, argv);
	if (ret &amp;lt; 0)
		rte_exit(EXIT_FAILURE, &quot;Invalid L2FWD arguments\n&quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;（其实真正做到分割的原因是系统函数getopt以及getopt_long，这些处理命令行参数的函数，处理到“–”时就会停止，所以这一机制可以被用来做多段参数）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;2创建内存池&quot;&gt;2.创建内存池&lt;/h2&gt;
&lt;p&gt;由于DPDK在使用前需要分配大页，所以实际创建内存池时就是从这些已分配的大页中创建。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;l2fwd_pktmbuf_pool = rte_pktmbuf_pool_create(&quot;mbuf_pool&quot;, NB_MBUF,
		MEMPOOL_CACHE_SIZE, 0, RTE_MBUF_DEFAULT_BUF_SIZE,
		rte_socket_id());
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;其中参数包括cache_size、priv_size、data_room_size，以及在哪个socket上分配。这里的socket不是网络中的套接字，而是numa架构的socket。
numa架构是多核技术发展的产物，在传统结构上，每个处理器都是通过系统总线访问内存，访存时间开销一致。而numa架构里，每个socket上有数个node，每个node又包括数个core。每个socket有自己的内存，每个socket里的处理器访问自己内存的速度最快，访问其他socket的内存则比较慢，如下图所示[1]。因此我们在创建缓冲区的时候就需要充分考虑到内存位置对性能的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-512e372d625e2ee3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;numa架构简单示意图.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3设置二层转发目的端口&quot;&gt;3.设置二层转发目的端口&lt;/h2&gt;

&lt;p&gt;对每个端口，先初始化设置他们的目的端口都是0，然后用一个for循环来让端口两两互为目的端口。例如：0号端口的目的端口是1,1号端口的目的端口是0；2号端口的目的端口是3，3号端口的目的端口是2……这里我们也可以修改成我们想要的转发规则。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (portid = 0; portid &amp;lt; nb_ports; portid++) {
		/* skip ports that are not enabled */
		if ((l2fwd_enabled_port_mask &amp;amp; (1 &amp;lt;&amp;lt; portid)) == 0)
			continue;

		if (nb_ports_in_mask % 2) {
			l2fwd_dst_ports[portid] = last_port;
			l2fwd_dst_ports[last_port] = portid;
		}
		else
			last_port = portid;

		nb_ports_in_mask++;

        //获取端口的名字、发送队列、接收队列等信息，主要就是填充每个端口的dev_info结构体
		rte_eth_dev_info_get(portid, &amp;amp;dev_info);
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;以及最后填充了一下每个端口的结构体里有关该端口的各种信息&lt;/p&gt;

&lt;h2 id=&quot;4为每个端口分配逻辑核&quot;&gt;4.为每个端口分配逻辑核&lt;/h2&gt;

&lt;p&gt;我们在命令行输入的参数有一个q，指的就是每个逻辑核最多可以用来处理几个端口，在这里绑定核的时候，就会执行这方面的检查。while后面的语句是寻找一个可同的逻辑核，在不超过最大核数量（128）的基础上，从0开始，看每个核是否超过了设置的每个核绑定几个端口数限制，如果没有当前循环的这个端口就可以绑定该核。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (portid = 0; portid &amp;lt; nb_ports; portid++) {
		...
		while (rte_lcore_is_enabled(rx_lcore_id) == 0 ||
		       lcore_queue_conf[rx_lcore_id].n_rx_port ==
		       l2fwd_rx_queue_per_lcore) {
			rx_lcore_id++;
			if (rx_lcore_id &amp;gt;= RTE_MAX_LCORE)
				rte_exit(EXIT_FAILURE, &quot;Not enough cores\n&quot;);
		}

		//绑定该核
		if (qconf != &amp;amp;lcore_queue_conf[rx_lcore_id])
			/* Assigned a new logical core in the loop above. */
			qconf = &amp;amp;lcore_queue_conf[rx_lcore_id];
        ...
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;实际的绑定就是在这个核处理的端口列表中加上当前这个端口，然后该核绑定的端口数加1。&lt;/p&gt;

&lt;h2 id=&quot;5初始化每个端口&quot;&gt;5.初始化每个端口&lt;/h2&gt;

&lt;p&gt;其中fflush函数是清除缓冲区的作用，会强迫未写入磁盘的内容立即写入。这部分比较简单，直接上源码。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (portid = 0; portid &amp;lt; nb_ports; portid++) {

		...

		//清除读写缓冲区
		fflush(stdout);
		//配置端口，将一些配置写进设备dev的一些字段，以及检查设备支持什么类型的中断、支持的包大小
		ret = rte_eth_dev_configure(portid, 1, 1, &amp;amp;port_conf);

		...

        //获取设备的MAC地址，写在后一个参数里
		rte_eth_macaddr_get(portid,&amp;amp;l2fwd_ports_eth_addr[portid]);

		/* init one RX queue */
		//清除缓冲区
		fflush(stdout);
		//设置接收队列
		ret = rte_eth_rx_queue_setup(portid, 0, nb_rxd,
					     rte_eth_dev_socket_id(portid),
					     NULL,
					     l2fwd_pktmbuf_pool);
		if (ret &amp;lt; 0)
			rte_exit(EXIT_FAILURE, &quot;rte_eth_rx_queue_setup:err=%d, port=%u\n&quot;,
				  ret, (unsigned) portid);

		/* init one TX queue on each port */
		fflush(stdout);
		//设置发送队列
		ret = rte_eth_tx_queue_setup(portid, 0, nb_txd,
				rte_eth_dev_socket_id(portid),
				NULL);
		if (ret &amp;lt; 0)
			rte_exit(EXIT_FAILURE, &quot;rte_eth_tx_queue_setup:err=%d, port=%u\n&quot;,
				ret, (unsigned) portid);

		/* Initialize TX buffers */
		//每个端口分配接收缓冲区，根据numa架构的socket就近分配
		tx_buffer[portid] = rte_zmalloc_socket(&quot;tx_buffer&quot;,
				RTE_ETH_TX_BUFFER_SIZE(MAX_PKT_BURST), 0,
				rte_eth_dev_socket_id(portid));
		if (tx_buffer[portid] == NULL)
			rte_exit(EXIT_FAILURE, &quot;Cannot allocate buffer for tx on port %u\n&quot;,
					(unsigned) portid);

		//初始化接收缓冲区
		rte_eth_tx_buffer_init(tx_buffer[portid], MAX_PKT_BURST);

        //设置接收缓冲区的err_callback
		ret = rte_eth_tx_buffer_set_err_callback(tx_buffer[portid],
				rte_eth_tx_buffer_count_callback,
				&amp;amp;port_statistics[portid].dropped);
		if (ret &amp;lt; 0)
				rte_exit(EXIT_FAILURE, &quot;Cannot set error callback for &quot;
						&quot;tx buffer on port %u\n&quot;, (unsigned) portid);

		/* Start device */
		//启用端口
		ret = rte_eth_dev_start(portid);
		if (ret &amp;lt; 0)
			rte_exit(EXIT_FAILURE, &quot;rte_eth_dev_start:err=%d, port=%u\n&quot;,
				  ret, (unsigned) portid);

		printf(&quot;done: \n&quot;);

		rte_eth_promiscuous_enable(portid);
        //打印端口MAC地址
		printf(&quot;Port %u, MAC address: %02X:%02X:%02X:%02X:%02X:%02X\n\n&quot;,
				(unsigned) portid,
				l2fwd_ports_eth_addr[portid].addr_bytes[0],
				l2fwd_ports_eth_addr[portid].addr_bytes[1],
				l2fwd_ports_eth_addr[portid].addr_bytes[2],
				l2fwd_ports_eth_addr[portid].addr_bytes[3],
				l2fwd_ports_eth_addr[portid].addr_bytes[4],
				l2fwd_ports_eth_addr[portid].addr_bytes[5]);

		/* initialize port stats */
		//初始化端口数据，就是后面要打印的，接收、发送、drop的包数
		memset(&amp;amp;port_statistics, 0, sizeof(port_statistics));
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;6任务分发&quot;&gt;6.任务分发&lt;/h2&gt;

&lt;p&gt;这里就是DPDK程序最熟悉的任务分发函数了，每个slave从线程启动后运行的函数是l2fwd_launch_one_lcore：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rte_eal_mp_remote_launch(l2fwd_launch_one_lcore, NULL, CALL_MASTER);
	RTE_LCORE_FOREACH_SLAVE(lcore_id) {
		if (rte_eal_wait_lcore(lcore_id) &amp;lt; 0) {
			ret = -1;
			break;
		}
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;而l2fwd_launch_one_lcore实际上运行的是l2fwd_main_loop，这就是上面说的从线程循环了&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int
l2fwd_launch_one_lcore(__attribute__((unused)) void *dummy)
{
	l2fwd_main_loop();
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;7从线程循环&quot;&gt;7.从线程循环&lt;/h2&gt;

&lt;p&gt;这部分就直接附上注释的源码吧，注释有点调皮~&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static void
l2fwd_main_loop(void)
{
	
    ...

	//获取自己的lcore_id
	lcore_id = rte_lcore_id();
	qconf = &amp;amp;lcore_queue_conf[lcore_id];

	//分配后多余的lcore，无事可做，orz
	if (qconf-&amp;gt;n_rx_port == 0) {
		RTE_LOG(INFO, L2FWD, &quot;lcore %u has nothing to do\n&quot;, lcore_id);
		return;
	}

    //有事做的核，很开心的进入了主循环~
	RTE_LOG(INFO, L2FWD, &quot;entering main loop on lcore %u\n&quot;, lcore_id);

	...

	//直到发生了强制退出，在这里就是ctrl+c或者kill了这个进程
	while (!force_quit) {

		cur_tsc = rte_rdtsc();

		/*
		 * TX burst queue drain
		 */
		//计算时间片
		diff_tsc = cur_tsc - prev_tsc;
		//过了100us，把发送buffer里的报文发出去
		if (unlikely(diff_tsc &amp;gt; drain_tsc)) {

			for (i = 0; i &amp;lt; qconf-&amp;gt;n_rx_port; i++) {

				portid = l2fwd_dst_ports[qconf-&amp;gt;rx_port_list[i]];
				buffer = tx_buffer[portid];

				sent = rte_eth_tx_buffer_flush(portid, 0, buffer);
				if (sent)
					port_statistics[portid].tx += sent;

			}

			//到了时间片了打印各端口的数据
			/* if timer is enabled */
			if (timer_period &amp;gt; 0) {

				/* advance the timer */
				timer_tsc += diff_tsc;

				/* if timer has reached its timeout */
				if (unlikely(timer_tsc &amp;gt;= timer_period)) {

					/* do this only on master core */
					//打印让master主线程来做
					if (lcore_id == rte_get_master_lcore()) {
						print_stats();
						/* reset the timer */
						timer_tsc = 0;
					}
				}
			}

			prev_tsc = cur_tsc;
		}

		/*
		 * Read packet from RX queues
		 */
		//没有到发送时间片的话，读接收队列里的报文
		for (i = 0; i &amp;lt; qconf-&amp;gt;n_rx_port; i++) {

			portid = qconf-&amp;gt;rx_port_list[i];
			nb_rx = rte_eth_rx_burst((uint8_t) portid, 0,
						 pkts_burst, MAX_PKT_BURST);

			//计数，收到的报文数
			port_statistics[portid].rx += nb_rx;

			for (j = 0; j &amp;lt; nb_rx; j++) {
				m = pkts_burst[j];
				rte_prefetch0(rte_pktmbuf_mtod(m, void *));
				//updating mac地址以及目的端口发送buffer满了的话，尝试发送
				l2fwd_simple_forward(m, portid);
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###值得注意的小trick&lt;/p&gt;

&lt;p&gt;程序中强制退出，是自己写的一个信号量，包括两种操作，即在ctrl+c或者kill了这个进程的时候，会触发：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static void
signal_handler(int signum)
{
    if (signum == SIGINT || signum == SIGTERM) {
        printf(&quot;\n\nSignal %d received, preparing to exit...\n&quot;,
                signum);
        force_quit = true;
    }
}

signal(SIGINT, signal_handler);
signal(SIGTERM, signal_handler);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;signal_handler函数第一个参数signum：指明了所要处理的信号类型，它可以取除了SIGKILL和SIGSTOP外的任何一种信号。 　 
第二个参数handler：描述了与信号关联的动作，它可以取以下三种值： 
&lt;strong&gt;1. SIG_IGN&lt;/strong&gt;
这个符号表示忽略该信号。 
&lt;strong&gt;2. SIG_DFL&lt;/strong&gt;　　
这个符号表示恢复对信号的系统默认处理。不写此处理函数默认也是执行系统默认操作。
&lt;strong&gt;3. sighandler_t类型的函数指针&lt;/strong&gt;
此函数必须在signal()被调用前申明，handler中为这个函数的名字。当接收到一个类型为sig的信号时，就执行handler 所指定的函数。（int）signum是传递给它的唯一参数。执行了signal()调用后，进程只要接收到类型为sig的信号，不管其正在执行程序的哪一部分，就立即执行func()函数。当func()函数执行结束后，控制权返回进程被中断的那一点继续执行。&lt;/p&gt;

&lt;p&gt;在该函数中就是第三种，所以当我们退出是ctrl+c不是直接将进程杀死，而是会将force_quit置为true，让程序自然退出，这样程序就来得及完成最后退出之前的操作。&lt;/p&gt;

&lt;p&gt;图引用：&lt;/p&gt;

&lt;p&gt;[1].&lt;a href=&quot;http://www.cnblogs.com/cenalulu/p/4358802.html&quot;&gt;http://www.cnblogs.com/cenalulu/p/4358802.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Sep 2017 19:03:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/dpdk/2017/09/22/DPDK-l2fwd.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/dpdk/2017/09/22/DPDK-l2fwd.html</guid>
        
        
        <category>DPDK</category>
        
      </item>
    
      <item>
        <title>qemu内存管理以及vhost-user的协商机制下的前后端内存布局</title>
        <description>&lt;p&gt;&lt;strong&gt;概括来说，qemu和KVM在内存管理上的关系就是：在虚拟机启动时，qemu在qemu进程地址空间申请内存，即内存的申请是在用户空间完成的。通过kvm提供的API，把地址信息注册到KVM中，这样KVM中维护有虚拟机相关的slot，这些slot构成了一个完整的虚拟机物理地址空间。slot中记录了其对应的HVA，页面数、起始GPA等，利用它可以把一个GPA转化成HVA，这正是KVM维护EPT的技术核心。整个内存虚拟化可以分为两部分：qemu部分和kvm部分。qemu完成内存的申请，kvm实现内存的管理。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-1e105a126e1431ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;qemu与KVM内存管理的分工.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;qemu中地址空间分两部分，两个全局变量system_memory和system_IO，其中system_memory是所有memory_region的父object，他们只负责管理内存。&lt;/li&gt;
  &lt;li&gt;在KVM中，也有两个全局变量address_space_memory和address_space_memory_IO，与qemu中的memory_region对应，只有将HVA和GPA的对应关系注册到KVM模块的memslot，才可以生效成为EPT。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-9b28de7bdb6207a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;system_memory和address_space_memory等的关系.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在qemu 2.9的前端virtio和dpdk17.05的后端vhost-user构成的虚拟队列中，会率先通过socket建立连接，将qemu中virtio的内存布局传给vhost，vhost收到包（该消息机制有自己的协议，暂称为msg）后，分析其中的信息，这里面通信包含一套自己写的协议。包含以下内容，均是在刚建立连接时候传递的：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static const char *vhost_message_str[VHOST_USER_MAX] = {
    [VHOST_USER_NONE] = &quot;VHOST_USER_NONE&quot;,
    [VHOST_USER_GET_FEATURES] = &quot;VHOST_USER_GET_FEATURES&quot;,
    [VHOST_USER_SET_FEATURES] = &quot;VHOST_USER_SET_FEATURES&quot;,
    [VHOST_USER_SET_OWNER] = &quot;VHOST_USER_SET_OWNER&quot;,
    [VHOST_USER_RESET_OWNER] = &quot;VHOST_USER_RESET_OWNER&quot;,
    [VHOST_USER_SET_MEM_TABLE] = &quot;VHOST_USER_SET_MEM_TABLE&quot;,
    [VHOST_USER_SET_LOG_BASE] = &quot;VHOST_USER_SET_LOG_BASE&quot;,
    [VHOST_USER_SET_LOG_FD] = &quot;VHOST_USER_SET_LOG_FD&quot;,
    [VHOST_USER_SET_VRING_NUM] = &quot;VHOST_USER_SET_VRING_NUM&quot;,
    [VHOST_USER_SET_VRING_ADDR] = &quot;VHOST_USER_SET_VRING_ADDR&quot;,
    [VHOST_USER_SET_VRING_BASE] = &quot;VHOST_USER_SET_VRING_BASE&quot;,
    [VHOST_USER_GET_VRING_BASE] = &quot;VHOST_USER_GET_VRING_BASE&quot;,
    [VHOST_USER_SET_VRING_KICK] = &quot;VHOST_USER_SET_VRING_KICK&quot;,
    [VHOST_USER_SET_VRING_CALL] = &quot;VHOST_USER_SET_VRING_CALL&quot;,
    [VHOST_USER_SET_VRING_ERR]  = &quot;VHOST_USER_SET_VRING_ERR&quot;,
    [VHOST_USER_GET_PROTOCOL_FEATURES]  = &quot;VHOST_USER_GET_PROTOCOL_FEATURES&quot;,
    [VHOST_USER_SET_PROTOCOL_FEATURES]  = &quot;VHOST_USER_SET_PROTOCOL_FEATURES&quot;,
    [VHOST_USER_GET_QUEUE_NUM]  = &quot;VHOST_USER_GET_QUEUE_NUM&quot;,
    [VHOST_USER_SET_VRING_ENABLE]  = &quot;VHOST_USER_SET_VRING_ENABLE&quot;,
    [VHOST_USER_SEND_RARP]  = &quot;VHOST_USER_SEND_RARP&quot;,
    [VHOST_USER_NET_SET_MTU]  = &quot;VHOST_USER_NET_SET_MTU&quot;,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;其中我们最关心的就是vhost_user_set_mem_table:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int
vhost_user_set_mem_table(struct virtio_net *dev, struct VhostUserMsg *pmsg)
{
	...
	for (i = 0; i &amp;lt; memory.nregions; i++) {
		fd  = pmsg-&amp;gt;fds[i];
		reg = &amp;amp;dev-&amp;gt;mem-&amp;gt;regions[i];

		reg-&amp;gt;guest_phys_addr = memory.regions[i].guest_phys_addr;
		reg-&amp;gt;guest_user_addr = memory.regions[i].userspace_addr;
		reg-&amp;gt;size            = memory.regions[i].memory_size;
		reg-&amp;gt;fd              = fd;

		mmap_offset = memory.regions[i].mmap_offset;
		mmap_size   = reg-&amp;gt;size + mmap_offset;

		/* mmap() without flag of MAP_ANONYMOUS, should be called
		 * with length argument aligned with hugepagesz at older
		 * longterm version Linux, like 2.6.32 and 3.2.72, or
		 * mmap() will fail with EINVAL.
		 *
		 * to avoid failure, make sure in caller to keep length
		 * aligned.
		 */
		alignment = get_blk_size(fd);
		if (alignment == (uint64_t)-1) {
			RTE_LOG(ERR, VHOST_CONFIG,
				&quot;couldn't get hugepage size through fstat\n&quot;);
			goto err_mmap;
		}
		mmap_size = RTE_ALIGN_CEIL(mmap_size, alignment);

		mmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE,
				 MAP_SHARED | MAP_POPULATE, fd, 0);
        //对每个region调用mmap映射共享内存
		if (mmap_addr == MAP_FAILED) {
			RTE_LOG(ERR, VHOST_CONFIG,
				&quot;mmap region %u failed.\n&quot;, i);
			goto err_mmap;
		}

	...
	return 0;

err_mmap:
	free_mem_region(dev);
	rte_free(dev-&amp;gt;mem);
	dev-&amp;gt;mem = NULL;
	return -1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;另外，我们在实际运行系统的过程中发现，qemu的内存布局和vhost端的内存布局，虽是通过共享内存建立的，但是既不是一整块内存映射，也不是通过零碎的region一小块一小块的映射。它们的内存布局如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-190ec713c4c44f1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;virtio前后端的内存布局.png&quot; /&gt;
在vhost这边只有两块region，而且像是将前端的内存region做了一个聚合得到的。回归代码，发现消息传递之前，传递的并非是memory_region变量，而是memory_region_section，在qemu的vhost_set_memory函数中，有这样一个操作：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if (add) {
        /* Add given mapping, merging adjacent regions if any */
        vhost_dev_assign_memory(dev, start_addr, size, (uintptr_t)ram);
    } else {
        /* Remove old mapping for this memory, if any. */
        vhost_dev_unassign_memory(dev, start_addr, size);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;将毗邻的memory_region合并了，这样就解释的通了。因为memory_region是一个树状结构，且有包含关系在里面，所以如果一个个传递，vhost里面用for循环进行映射到自己地址空间，效率低下，而且大多数内存vhost用不到，没有必要这么细分。&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Sep 2017 18:19:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2017/09/12/qemu-memory-manager.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2017/09/12/qemu-memory-manager.html</guid>
        
        
        <category>virtualized-network-IO</category>
        
      </item>
    
      <item>
        <title>DPDK+OVS+QEMU搭建vhost-user实验环境</title>
        <description>&lt;p&gt;目前在virtio后端驱动方面性能最好的是用户态的vhost-user，而DPDK又是用户态vhost实现里使用最广泛的。下面介绍一下怎么搭建这样一个vhost-user实验环境。我们这里使用的全部是最新的版本（ovs2.8+DPDK17.05+qemu2.9.93）.&lt;/p&gt;

&lt;h2 id=&quot;1由于涉及到虚拟化先检查计算机是否开启了虚拟化&quot;&gt;1.由于涉及到虚拟化，先检查计算机是否开启了虚拟化&lt;/h2&gt;
&lt;p&gt;先确保计算机bios打开了Intel-VT，然后检查grub选项是否开启了IOMMU，vim打开/etc/default/grub，在GRUB_CMDLINE_LINUX_DEFAULT后的引号里面加上一句：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iommu=pt intel_iommu=on default_hugepagesz=1G hugepagesz=1G hugepages=8
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;如图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-7c2f5638ac3e0d1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;grub设置.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里直接添加了大页的配置，DPDK的大页配置支持2MB大页和1GB大页，使用的时候根据具体情况而定，这里我们分配了8个1GB的大页。&lt;/p&gt;

&lt;h2 id=&quot;2然后编译dpdk&quot;&gt;2.然后编译DPDK&lt;/h2&gt;

&lt;p&gt;DPDK默认编译成.a的静态链接库，但也可以修改DPDK文件夹下的config/common_base文件里的CONFIG_RTE_BUILD_SHARED_LIB=y （但有一点要注意的就是，common_base是生成所有平台下编译选项的基础，直接修改也会影响其他target下的编译，也可以仅仅修改生成的本平台下的config文件，或者lib库编译里面的build/.config）来编译成动态链接库。该文件是所有架构下最基础引用的配置文件，在文件中可以设置某些库不编译，以及是否编译允许debug等。&lt;/p&gt;

&lt;p&gt;之后就和官网教程一样，设置编译目标环境，config等&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make config T=x86_64-native-linuxapp-gcc

sed -ri 's,(PMD_PCAP=).*,\1y,' build/.config

make

make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;make完成之后，如果选择的是编译成静态链接库，这段无视。如果编译成动态链接库，去build/lib文件夹下把整个文件夹里面的.so动态链接库文件全部复制到/usr/lib/dpdk/文件夹下，没有文件夹就建一个。然后在/etc/ld.so.conf.d/文件夹下建一个dpdk.conf文件，编辑dpdk.conf，写一句话：/usr/lib/dpdk。保存退出，命令行输入ldconfig，使系统自动加载的动态链接库生效。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;也可以不这么做，但每次运行都需要改下环境变量，LD_LIBRARY_PATH指向我们刚刚编译好的lib文件夹。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;3编译ovs&quot;&gt;3.编译OVS&lt;/h2&gt;
&lt;p&gt;在ovs文件夹内configure时加上–with-dpdk选项，即可与DPDK关联。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure --with-dpdk=$DPDK_BUILD
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;DPDK_BUILD就是之前编译dpdk之后产生的build文件夹路径。
然后&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make

make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;安装完成之后，就可以启动ovs了，需要DPDK实现绑定大页和网卡，以后每次启动ovs只需要最下面两个命令启动ovs最重要两个进程即可：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;modprobe vfio-pci

chmod a+x /dev/vfio

chmod 0666 /dev/vfio/*

/home/yangye/ovs-dpdk/dpdk-stable-17.05.1/usertools/dpdk-devbind.py --bind=vfio-pci eth4

mount -t hugetlbfs -o pagesize=1G none /dev/hugepages

mkdir -p /usr/local/etc/openvswitch #刚装完OVS需要新创建这个目录，以后用的时候不用
ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema  #利用ovsdb-tool创建ovsdb数据库

ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock --remote=db:Open_vSwitch,Open_vSwitch,manager_options --private-key=db:Open_vSwitch,SSL,private_key --certificate=db:Open_vSwitch,SSL,certificate --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert --pidfile --detachovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true  #启动ovsdb进程

ovs-vswitchd --pidfile --detach   #启动ovs-vswitch进程
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;其中的网卡绑定结合实际选择网卡，这样ovsdb和ovs-switchd进程都启动了，就可以执行ovs命令了。
建立一个虚拟网桥用于和qemu前端进行socket通信。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev

ovs-vsctl add-port br0 vhost-user-1 -- set Interface vhost-user-1 type=dpdkvhostuserclient options:vhost-server-path=&quot;/tmp/sock0&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;这步可能有各种错误，耐心找下都是什么原因，大多数都是和内存有关，比如第一次有可能因为未来得及分配内存建立端口时候不成功，删除了这个端口再建一次&lt;/em&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;建立网桥和端口有没有成功，可以用ovs-vsctl show命令查看，正确的状态如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;d9fd8d79-b6c0-4f13-8369-4755085471ba

   Bridge &quot;br0&quot;

       Port &quot;br0&quot;

           Interface &quot;br0&quot;

               type: internal

       Port &quot;vhost-user-1&quot;

           Interface &quot;vhost-user-1&quot;

               type: dpdkvhostuserclient

               options: {vhost-server-path=&quot;/tmp/sock0&quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;至此，后端驱动vhost以及它之上的交换机已经启动了，处于等待状态。（但是qemu 2.7以上才支持重连功能）&lt;/p&gt;

&lt;h2 id=&quot;4再来安装qemu&quot;&gt;4.再来安装qemu&lt;/h2&gt;
&lt;p&gt;qemu安装比较简单，直接configure、make、make install即可。
创建镜像，必须要qcow2格式：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qemu-img create -f qcow2 virtual.img 20G
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;安装虚拟机，设置了端口50，可以用VNC远处连接虚拟机：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qemu-system-x86_64 -m 2048 --enable-kvm -boot d -hda /var/iso/virtual.img -cdrom /var/iso/ubuntu-16.04.2-desktop-amd64.iso -vnc 0.0.0.0:50
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;下载一个VNC软件，像tightVNC，就可以输入”主机ip:50”来连接该虚拟机，然后进入安装流程，安装完毕后退出。&lt;/p&gt;

&lt;p&gt;使用vhost-user作为网络接口，启动虚拟机：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qemu-system-x86_64 -machine accel=kvm -cpu host -smp sockets=2,cores=2,threads=2 -m 2048M -object memory-backend-file,id=mem,size=2048M,mem-path=/dev/hugepages,share=on -drive file=/var/iso/virtual.img -drive file=/opt/share.img,if=virtio -mem-prealloc -numa node,memdev=mem -vnc 0.0.0.0:50 --enable-kvm -chardev socket,id=char1,path=/tmp/sock0,server -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce -device virtio-net-pci,netdev=mynet1,id=net1,mac=00:00:00:00:00:01
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;虚拟机启动，vhost的前后端驱动也已连接。此时在/var/log/syslog中也能看到DPDK输出的日志信息，读取了前端qemu的内存布局。这样一个vhost-user环境就已经搭起来了，可以用TightVNC远程连接这个虚拟机了。&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Sep 2017 17:56:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2017/09/12/vhost-user-installer.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/virtualized-network-io/2017/09/12/vhost-user-installer.html</guid>
        
        
        <category>virtualized-network-IO</category>
        
      </item>
    
      <item>
        <title>linux下静态/动态函数库的原理和编写</title>
        <description>&lt;p&gt;一直想系统地了解下linux下神奇的动态函数库，这在大型的工程里面非常常见。需要把一些代码编译成自己的库，然后划分成模块调用。下面就是综合几篇非常优秀的博文，结合我自己的实践，算是一个整合吧。&lt;/p&gt;

&lt;h2 id=&quot;1-介绍&quot;&gt;1. 介绍&lt;/h2&gt;
&lt;p&gt;  
使用GNU的工具我们如何在linux下创建自己的程序函数库？一个“程序函数库”简单的说就是一个文件包含了一些编译好的代码和数据，这些编译好的代码和数据可以在事后供其他的程序使用。程序函数库可以使整个程序更加模块化，更容易重新编译，而且更方便升级。  
程序函数库可分为3种类型：静态函数库（static libraries）、共享函数库（shared libraries）、动态加载函数库（dynamically loaded libraries）： &lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;静态函数库&lt;/strong&gt;，是在程序执行前就加入到目标程序中去了。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;共享函数库&lt;/strong&gt;，则是在程序启动的时候加载到程序中，它可以被不同的程序共享。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;动态函数库&lt;/strong&gt;，可以在程序运行的任何时候动态的加载。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-静态函数库&quot;&gt;2. 静态函数库&lt;/h2&gt;

&lt;p&gt;静态函数库实际上就是简单的一个普通的目标文件的集合，一般来说习惯用“.a”作为文件的后缀。可以用ar这个程序来产生静态函数库文件。Ar是archiver的缩写。静态函数库现在已经不在像以前用得那么多了，主要是共享函数库与之相比较有很多的优势的原因。慢慢地，大家都喜欢使用共享函数库了。不过，在一些场所静态函数库仍然在使用，一来是保持一些与以前某些程序的兼容，二来它描述起来也比较简单。&lt;/p&gt;

&lt;p&gt;静态库函数允许程序员把程序link起来而不用重新编译代码，节省了重新编译代码的时间。不过，在今天这么快速的计算机面前，一般的程序的重新编译也花费不了多少时间，所以这个优势已经不是像它以前那么明显了。静态函数库对开发者来说还是很有用的，例如你想把自己提供的函数给别人使用，但是又想对函数的源代码进行保密，你就可以给别人提供一个静态函数库文件。理论上说，使用ELF格式的静态库函数生成的代码可以比使用共享函数库（或者动态函数库）的程序运行速度上快一些，大概1－5％。&lt;/p&gt;

&lt;p&gt;创建一个静态函数库文件，或者往一个已经存在地静态函数库文件添加新的目标代码，可以用下面的命令：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ar rcs my_library.a file1.o file2.o&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个例子中是把目标代码file1.o和file2.o加入到my_library.a这个函数库文件中，如果my_library.a不存在则创建一个新的文件。在用ar命令创建静态库函数的时候，还有其他一些可以选择的参数，可以参加ar的使用帮助。这里不再赘述。&lt;/p&gt;

&lt;p&gt;一旦你创建了一个静态函数库，你可以使用它了。你可以把它作为你编译和连接过程中的一部分用来生成你的可执行代码。如果你用gcc来编译产生可执行代码的话，你可以用“-l”参数来指定这个库函数。你也可以用ld来做，使用它的“-l”和“-L”参数选项。&lt;/p&gt;

&lt;p&gt;例如，这里有一个头文件：so_test.h，三个.c文件：test_a.c、test_b.c、test_c.c，（下面的共享库和动态链接库也是基于这个例子），我们将这几个文件编译成一个静态库：libtest.a。&lt;/p&gt;

&lt;p&gt;so_test.h：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;stdio.h&quot;

void test_a();

void test_b();

void test_c();

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;test_a.c：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;so_test.h&quot;

void test_a()
{

printf(&quot;this is test_a!\n&quot;);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;test_b.c：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;so_test.h&quot;

void test_b()
{

printf(&quot;this is test_b!\n&quot;);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;test_c.c：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;so_test.h&quot;

void test_c()
{

printf(&quot;this is test_c!\n&quot;);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;先将三个.c文件编译成.o文件&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcc -c test_a.c
gcc -c test_b.c
gcc -c test_c.c
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;然后用命令创建静态链接库文件：
&lt;code class=&quot;highlighter-rouge&quot;&gt;ar rcs libtest.a test_a.o test_b.o test_c.o&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这样我们成功生成了一个自己的静态链接库libtest.a，下面我们通过一个程序来调用这个库里的函数。&lt;/p&gt;

&lt;p&gt;test.c：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;so_test.h&quot;

int main()
{

test_a();

test_b();

test_c();

return 0;

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;用gcc命令编译链接：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcc test.c -L. -ltest -o test
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-c5e19984bf2fb76c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;编译运行.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-共享函数库&quot;&gt;3. 共享函数库&lt;/h2&gt;

&lt;p&gt;共享函数库中的函数是在当一个可执行程序在启动的时候被加载。如果一个共享函数库正常安装，所有的程序在重新运行的时候都可以自动加载最新的函数库中的函数。对于Linux系统还有更多可以实现的功能：        &lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;升级了函数库但是仍然允许程序使用老版本的函数库。       &lt;/li&gt;
  &lt;li&gt;当执行某个特定程序的时候可以覆盖某个特定的库或者库中指定的函数。       &lt;/li&gt;
  &lt;li&gt;可以在库函数被使用的过程中修改这些函数库。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;31-soname和real-name&quot;&gt;3.1. “soname”和“real name”&lt;/h3&gt;

&lt;p&gt;一些约定如果你要编写的共享函数库支持所有有用的特性，你在编写的过程中必须遵循一系列约定。你必须理解库的不同的名字间的区别，例如它的“soname”和“real name”之间的区别和它们是如何相互作用的。你同样还要知道你应该把这些库函数放在你文件系统的什么位置等等。下面我们具体看看这些问题。 &lt;/p&gt;

&lt;h4 id=&quot;311-共享库的命名&quot;&gt;3.1.1. 共享库的命名&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;每个共享函数库都有个特殊的名字，称作“soname”。soname名字命名必须以“lib”作为前缀，然后是函数库的名字，然后是“.so”，最后是版本号信息。不过有个特例，就是非常底层的C库函数都不是以lib开头这样命名的。    &lt;/li&gt;
  &lt;li&gt;每个共享函数库都有一个真正的名字（“real name”），它是包含真正库函数代码的文件。真名有一个主版本号，和一个发行版本号。最后一个发行版本号是可选的，可以没有。主版本号和发行版本号使你可以知道你到底是安装了什么版本的库函数。另外，还有一个名字是编译器编译的时候需要的函数库的名字，这个名字就是简单的soname名字，而不包含任何版本号信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;管理共享函数库的关键是区分好这些名字。当可执行程序需要在自己的程序中列出这些他们需要的共享库函数的时候，它只要用soname就可以了；反过来，当你要创建一个新的共享函数库的时候，你要指定一个特定的文件名，其中包含很细节的版本信息。当你安装一个新版本的函数库的时候，你只要先将这些函数库文件拷贝到一些特定的目录中，运行ldconfig这个实用就可以。ldconfig检查已经存在的库文件，然后创建soname的符号链接到真正的函数库，同时设置/etc/ld.so.cache这个缓冲文件。这个我们稍后再讨论。&lt;/p&gt;

&lt;p&gt;ldconfig并不设置链接的名字，通常的做法是在安装过程中完成这个链接名字的建立，一般来说这个符号链接就简单的指向最新的soname或者最新版本的函数库文件。最好把这个符号链接指向soname，因为通常当你升级你的库函数后，你就可以自动使用新版本的函数库类。&lt;/p&gt;

&lt;p&gt;我们来举例看看：/usr/lib/libreadline.so.3 是一个完全的完整的soname，ldconfig可以设置一个符号链接到其他某个真正的函数库文件，例如是/usr/lib/libreadline.so.3.0。同时还必须有一个链接名字，例如 /usr/lib/libreadline.so就是一个符号链接指向/usr/lib/libreadline.so.3。&lt;/p&gt;

&lt;h4 id=&quot;312-文件系统中函数库文件的位置&quot;&gt;3.1.2. 文件系统中函数库文件的位置&lt;/h4&gt;

&lt;p&gt;共享函数库文件必须放在一些特定的目录里，这样通过系统的环境变量设置，应用程序才能正确的使用这些函数库。大部分的源码开发的程序都遵循GNU的一些标准，我们可以看info帮助文件获得相信的说明，info信息的位置是：info:standards#Directory_Variables。GNU标准建议所有的函数库文件都放在/usr/local/lib目录下，而且建议命令可执行程序都放在/usr/local/bin目录下。这都是一些习惯问题，可以改变的。 &lt;/p&gt;

&lt;p&gt;文件系统层次化标准FHS（Filesystem Hierarchy Standard）（&lt;a href=&quot;http://www.pathname.com/fhs&quot;&gt;http://www.pathname.com/fhs&lt;/a&gt;）规定了在一个发行包中大部分的函数库文件应该安装到/usr/lib目录下，但是如果某些库是在系统启动的时候要加载的，则放到/lib目录下，而那些不是系统本身一部分的库则放到/usr/local/lib下面。 &lt;/p&gt;

&lt;p&gt;上面两个路径的不同并没有本质的冲突。GNU提出的标准主要对于开发者开发源码的，而FHS的建议则是针对发行版本的路径的。具体的位置信息可以看/etc/ld.so.conf里面的配置信息。&lt;/p&gt;

&lt;h3 id=&quot;32-这些函数库如何使用&quot;&gt;3.2. 这些函数库如何使用&lt;/h3&gt;

&lt;p&gt;在基于GNU glibc的系统里，包括所有的linux系统，启动一个ELF格式的二进制可执行文件会自动启动和运行一个program loader。对于Linux系统，这个loader的名字是/lib/ld-linux.so.X（X是版本号）。这个loader启动后，反过来就会load所有的其他本程序要使用的共享函数库。&lt;/p&gt;

&lt;p&gt;到底在哪些目录里查找共享函数库呢？这些定义缺省的是放在/etc/ld.so.conf文件里面，我们可以修改这个文件，加入我们自己的一些特殊的路径要求。大多数RedHat系列的发行包的/etc/ld.so.conf文件里面不包括/usr/local/lib这个目录，如果没有这个目录的话，我们可以修改/etc/ld.so.conf，自己手动加上这个条目。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果你想覆盖某个库中的一些函数，用自己的函数替换它们，同时保留该库中其他的函数的话，你可以在 /etc/ld.so.preload中加入你想要替换的库（.o结尾的文件），这些preloading的库函数将有优先加载的权利。
当程序启动的时候搜索所有的目录显然会效率很低，于是Linux系统实际上用的是一个高速缓冲的做法。ldconfig缺省情况下读出/etc/ld.so.conf相关信息，然后设置适当地符号链接，然后写一个cache到 /etc/ld.so.cache这个文件中，而这个/etc/ld.so.cache则可以被其他程序有效的使用了。这样的做法可以大大提高访问函数库的速度。这就要求每次新增加一个动态加载的函数库的时候，就要运行ldconfig来更新这个cache，如果要删除某个函数库，或者某个函数库的路径修改了，都要重新运行ldconfig来更新这个cache。通常的一些包管理器在安装一个新的函数库的时候就要运行ldconfig。 &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;另外，FreeBSD使用cache的文件不一样。FreeBSD的ELF cache是/var/run/ld-elf.so.hints，而a.out的cache则是/var/run/ld.so.hints。它们同样是通过ldconfig来更新。&lt;/p&gt;

&lt;h3 id=&quot;33-环境变量&quot;&gt;3.3. 环境变量&lt;/h3&gt;

&lt;p&gt;各种各样的环境变量控制着一些关键的过程。例如你可以临时为你特定的程序的一次执行指定一个不同的函数库。Linux系统中，通常变量LD_LIBRARY_PATH就是可以用来指定函数库查找路径的，而且这个路径通常是在查找标准的路径之前查找。这个是很有用的，特别是在调试一个新的函数库的时候，或者在特殊的场合使用一个非标准的函数库的时候。环境变量LD_PRELOAD列出了所有共享函数库中需要优先加载的库文件，功能和/etc/ld.so.preload类似。这些都是有/lib/ld-linux.so这个loader来实现的。值得一提的是，LD_LIBRARY_PATH可以在大部分的UNIX-linke系统下正常起作用，但是并非所有的系统下都可以使用，例如HP－UX系统下，就是用SHLIB_PATH这个变量，而在AIX下则使用LIBPATH这个变量。&lt;/p&gt;

&lt;p&gt;LD_LIBRARY_PATH在开发和调试过程中经常大量使用，但是不应该被一个普通用户在安装过程中被安装程序修改，大家可以去参考(http://www.visi.com/%7Ebarr/ldpath.html), 这里有一个文档专门介绍为什么不使用LD_LIBRARY_PATH这个变量。&lt;/p&gt;

&lt;p&gt;事实上还有更多的环境变量影响着程序的调入过程，它们的名字通常就是以LD_或者RTLD_打头。大部分这些环境变量的使用的文档都是不全，通常搞得人头昏眼花的，如果要真正弄清楚它们的用法，最好去读loader的源码（也就是gcc的一部分）。&lt;/p&gt;

&lt;p&gt;允许用户控制动态链接函数库将涉及到setuid/setgid这个函数，如果特殊的功能需要的话。因此，GNU loader通常限制或者忽略用户对这些变量使用setuid和setgid。如果loader通过判断程序的相关环境变量判断程序的是否使用了setuid或者setgid，如果uid和euid不同，或者gid和egid部一样，那么loader就假定程序已经使用了setuid或者setgid，然后就大大的限制器控制这个老链接的权限。如果阅读GNU glibc的库函数源码，就可以清楚地看到这一点。特别的我们可以看elf/rtld.c和sysdeps/generic/dl-sysdep.c这两个文件。这就意味着如果你使得uid和gid与euid和egid分别相等，然后调用一个程序，那么这些变量就可以完全起效。&lt;/p&gt;

&lt;h3 id=&quot;34-创建一个共享函数库&quot;&gt;3.4. 创建一个共享函数库&lt;/h3&gt;

&lt;p&gt;下面通过一个例子来介绍如何生成一个动态库。还是与上面一样的代码。
将这test_a.c、test_b.c、test_c.c三个文件编译成一个动态库：libtest.so&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gcc test_a.c test_b.c test_c.c -fPIC -shared -o libtest.so&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这样我们成功生成了一个自己的动态链接库libtest.so，然后用gcc编译链接上我们刚才编译的动态库：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gcc test.c -L. -ltest -o test&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;生成了一个test的可执行文件，然而现在ldd命令还显示找不到库：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-0da864cfa7dbcc8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;ldd.png&quot; /&gt;
接下来可以用两种办法，一是移动这个.so文件到/usr/lib/文件夹中：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-f5ad057ca76b78b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;成功运行.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;还有一种是修改环境变量：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-aeff54832da7c677.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;修改环境变量.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-动态加载的函数库dynamically-loaded-dl-libraries&quot;&gt;4. 动态加载的函数库Dynamically Loaded (DL) Libraries&lt;/h2&gt;
&lt;p&gt; 
动态加载的函数库Dynamically loaded (DL) libraries是一类函数库，它可以在程序运行过程中的任何时间加载。它们特别适合在函数中加载一些模块和plugin扩展模块的场合，因为它可以在当程序需要某个plugin模块时才动态的加载。例如，Pluggable Authentication Modules(PAM)系统就是用动态加载函数库来使得管理员可以配置和重新配置身份验证信息。&lt;/p&gt;

&lt;p&gt;Linux系统下，DL函数库与其他函数库在格式上没有特殊的区别，我们前面提到过，它们创建的时候是标准的object格式。主要的区别就是这些函数库不是在程序链接的时候或者启动的时候加载，而是通过一个API来打开一个函数库，寻找符号表，处理错误和关闭函数库。通常C语言环境下，需要包含这个头文件。&lt;/p&gt;

&lt;p&gt;Linux中使用的函数和Solaris中一样，都是dlpoen()的API。当然不是所有的平台都使用同样的接口，例如HP-UX使用shl_load()机制，而Windows平台用另外的其他的调用接口。如果你的目的是使得你的代码有很强的移植性，你应该使用一些wrapping函数库，这样的wrapping函数库隐藏不同的平台的接口区别。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;一种方法是使用glibc函数库中的对动态加载模块的支持，它使用一些潜在的动态加载函数库界面使得它们可以夸平台使用。具体可以参考http://developer/.gnome.org/doc/API/glib/glib-dynamic-loading-of-modules.html 。
另外一个方法是使用libltdl，是GNU libtool的一部分，可以进一步参考CORBA相关资料。  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;41-dlopen&quot;&gt;4.1. dlopen()&lt;/h3&gt;

&lt;p&gt;dlopen函数打开一个函数库然后为后面的使用做准备。C语言原形是：&lt;/p&gt;

&lt;p&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;void * dlopen(const char *filename, int flag);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果文件名filename是以“/”开头，也就是使用绝对路径，那么dlopne就直接使用它，而不去查找某些环境变量或者系统设置的函数库所在的目录了。否则dlopen()就会按照下面的次序查找函数库文件：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;环境变量LD_LIBRARY指明的路径。&lt;/li&gt;
  &lt;li&gt;/etc/ld.so.cache中的函数库列表。&lt;/li&gt;
  &lt;li&gt;/lib目录，然后/usr/lib。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不过一些很老的a.out的loader则是采用相反的次序，也就是先查 /usr/lib，然后是/lib。    &lt;/p&gt;

&lt;p&gt;dlopen()函数中，参数flag的值必须是RTLD_LAZY或者RTLD_NOW，RTLD_LAZY的意思是resolve undefined symbols as code from the dynamic library is executed，而RTLD_NOW的含义是resolve all undefined symbols before dlopen() returns and fail if this cannot be done’。&lt;/p&gt;

&lt;p&gt;如果有好几个函数库，它们之间有一些依赖关系的话，例如X依赖Y，那么你就要先加载那些被依赖的函数。例如先加载Y，然后加载X。&lt;/p&gt;

&lt;p&gt;dlopen()函数的返回值是一个句柄，然后后面的函数就通过使用这个句柄来做进一步的操作。如果打开失败dlopen()就返回一个NULL。如果一个函数库被多次打开，它会返回同样的句柄。     如果一个函数库里面有一个输出的函数名字为_init,那么_init就会在dlopen()这个函数返回前被执行。我们可以利用这个函数在我的函数库里面做一些初始化的工作。我们后面会继续讨论这个问题的。  &lt;/p&gt;

&lt;h3 id=&quot;42-dlerror&quot;&gt;4.2. dlerror()&lt;/h3&gt;

&lt;p&gt;通过调用dlerror()函数，我们可以获得最后一次调用dlopen()，dlsym()，或者dlclose()的错误信息。 &lt;/p&gt;

&lt;h3 id=&quot;43-dlsym&quot;&gt;4.3. dlsym()&lt;/h3&gt;

&lt;p&gt;如果你加载了一个DL函数库而不去使用当然是不可能的了，使用一个DL函数库的最主要的一个函数就是dlsym()，这个函数在一个已经打开的函数库里面查找给定的符号。这个函数如下定义：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;void * dlsym(void *handle, char *symbol);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;函数中的参数handle就是由dlopen打开后返回的句柄，symbol是一个以NIL结尾的字符串。如果dlsym()函数没有找到需要查找的symbol，则返回NULL。如果你知道某个symbol的值不可能是NULL或者0，那么就很好，你就可以根据这个返回结果判断查找的symbol是否存在了；不过，如果某个symbol的值就是NULL，那么这个判断就有问题了。标准的判断方法是先调用dlerror()，清除以前可能存在的错误，然后调用dlsym()来访问一个symbol，然后再调用dlerror()来判断是否出现了错误。&lt;/p&gt;

&lt;h3 id=&quot;44-dlclose&quot;&gt;4.4. dlclose()&lt;/h3&gt;

&lt;p&gt;dlopen()函数的反过程就是dlclose()函数，dlclose()函数用力关闭一个DL函数库。Dl函数库维持一个资源利用的计数器，当调用dlclose的时候，就把这个计数器的计数减一，如果计数器为0，则真正的释放掉。真正释放的时候，如果函数库里面有_fini()这个函数，则自动调用_fini()这个函数，做一些必要的处理。Dlclose()返回0表示成功，其他非0值表示错误。&lt;/p&gt;

&lt;p&gt;比方说对上面我们编译好的libtest.so文件动态链接，代码如下：
test.c：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;dlfcn.h&amp;gt;

#define LIB_TEST_PATH &quot;./libtest.so&quot;

typedef void (*PRINT)();   //函数指针

int main()
{
    void *handle;
    PRINT print=NULL;
    char *error;


    handle = dlopen(LIB_TEST_PATH, RTLD_LAZY);   //handle句柄
    if(!handle){  
        fputs(dlerror(), stderr);  
        exit(1);  
    }

    *(void **) (&amp;amp;print) = dlsym(handle, &quot;test_a&quot;);  //获取库中的函数 

    if((error = dlerror()) != NULL){  
        fputs(error, stderr);  
        exit(1);  
    }

    (*print)();

    *(void **) (&amp;amp;print) = dlsym(handle, &quot;test_b&quot;); 
    (*print)();

    *(void **) (&amp;amp;print) = dlsym(handle, &quot;test_c&quot;);
    (*print)(); 

    dlclose(handle);
    exit(EXIT_SUCCESS);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;然后我们用gcc编译链接
&lt;code class=&quot;highlighter-rouge&quot;&gt;gcc -o test test.c -ldl&lt;/code&gt;
运行如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/5971286-6f7a36240a470187.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;编译运行.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;文章主要引用自：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;[1]. Linux动态链接库.so文件的创建与使用；&lt;a href=&quot;http://blog.csdn.net/ithomer/article/details/7346146&quot;&gt;http://blog.csdn.net/ithomer/article/details/7346146&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]. 编译Linux使用的.a库文件；&lt;a href=&quot;http://blog.csdn.net/zengraoli/article/details/40741373&quot;&gt;http://blog.csdn.net/zengraoli/article/details/40741373&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3]. 关于Linux下.so的介绍和编写过程；&lt;a href=&quot;http://blog.csdn.net/lwhsyit/article/details/2860964&quot;&gt;http://blog.csdn.net/lwhsyit/article/details/2860964&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 15 May 2017 01:52:39 +0800</pubDate>
        <link>http://localhost:4000https://ictyangye.github.io/operation-system/2017/05/14/linux-a-so.html</link>
        <guid isPermaLink="true">http://localhost:4000https://ictyangye.github.io/operation-system/2017/05/14/linux-a-so.html</guid>
        
        
        <category>operation-system</category>
        
      </item>
    
  </channel>
</rss>
